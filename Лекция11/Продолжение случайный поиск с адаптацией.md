...Продолжение случайный поиск с адаптацией

Мы имеем некую обучающую выборку. Матрица **Х** с **n** признаками и вектор **Y**, где **yϵ[1,2]**, в данном случае рассматриваем классификацию на 2 класса. **n** – может быть очень большим.

Нужно выбрать **k** **<** **n** наиболее информативных(полезных) для классификации признаков.

Самый простой, надежный метод — это перебрать все возможные признаки, например:

```
      3!100
120
```

Чтобы выбрать лучшую тройку, нужен критерий, например, проекции многомерного Фишера и вычисление для этих проекций одномерного Фишера:

<img src="/home/ekaterina/Desktop/AO/Лекция 10/20210507.jpg" style="zoom:10%;" />

Чем больше F, тем подсистема полезней.

Найдя такую подсистему, будем использовать ее для алгоритма классификации.

Если из 10 получилось **i(the_best)=3,6,9** признаки, их спроектируем на плоскость и получим

<img src="/home/ekaterina/Desktop/AO/Лекция 10/20210507(2).jpg" style="zoom:20%;" />

Мы счастливы, никаких случайных поисков и адаптаций.

Если в задаче есть кризис теплообмена по спектрам акустического сигнала, т.е. установлен ПЭП и с него измеряем акустический сигнал и вычисляем спектральную плотность мощности(СПМ).

<img src="/home/ekaterina/Desktop/AO/Лекция 10/20210507(3).jpg" style="zoom:20%;" />

Простое представление:

<img src="/home/ekaterina/Desktop/AO/Лекция 10/20210507(4).jpg" style="zoom:20%;" />

Совершенно нормальным является оценивание СПМ на 200 частотах в диапазоне нахождения сигнала(ссылаемся на матрицу **Х**):

<img src="/home/ekaterina/Desktop/AO/Лекция 10/20210507(5).jpg" style="zoom:20%;" />

<img src="/home/ekaterina/Desktop/AO/Лекция 10/20210507(6).jpg" style="zoom:20%;" />

Это плохо.

Нам всегда нужно выбрать из имеющихся признаков информативные. Решаем две задачи, когда выбираем подмножество:

1. Облегчить вычисление, легче обращать матрицу 10 на 10, чем 200 на 200.
2. Убрать мешающие признаки, которые нам не позволяют сделать правильную классификацию.

Ищем 10 наилучших признаков из 200 – это 2 на 10^16 всевозможных комбинаций

```
      10!200
2.245100431Е16
```

Это очень большое число, которое мы будем перебирать всю жизнь. Именно для такой ситуации у нас есть случайной поиск с адаптацией, когда мы должны не 2 на 10^16 перебирать, а наши **r*R**-испытаний, где **r**-число испытаний в группе, **R**-число групп испытаний.

Начинаем с равномерного распределения вероятности выбрать каждый признак.

<img src="/home/ekaterina/Desktop/AO/Лекция 10/20210507(7).jpg" style="zoom:25%;" />

После проведения **r** испытаний по **k** признакам **(k < n)**, поощряем попавшие в наилучшую группу **p->p+dp**, наказываем попавших в наихудшую группу **p->p-dp**.

Наилучшая или наихудшая группа по выбранному критерию качество, например, многомерный Фишер.

1. Повторяем **r** испытаний с новым распределением вероятности выбора каждого признака.
2. Наказываем и поощряем  
3. Повторяем 1 и 2 **R** раз.
4. Выбираем из **r*R** – систему признаков.

По поводу параметров алгоритма:

1) **k**-сколько признаков хотим(чем меньше, тем лучше, когда их 2 – это шикарно)

```
      2!200
19900
```

2) **dp**-поощрение/наказание

3) **r**-число испытаний в группе

4) **R**-число групп испытаний

**1)**

Основным методом исследования в Date Science

<img src="/home/ekaterina/Desktop/AO/Лекция 10/20210507(8).jpg" style="zoom:15%;" />

Для решения каждой задачи в анализе данных существует множество методов.

Почему их много? Почему нет одного? Потому что очень сильно зависит от характеристик задачи того или иного метода. Задача, например, научить ПК отличать крестики от кружочков.

<img src="/home/ekaterina/Desktop/AO/Лекция 10/20210507(9).jpg" style="zoom:15%;" />

Если перепробуем все методы, то они все ее решат.

Если мы не знаем, кто здоровый или больной, а задача стоит в анализе имеющихся данных на однородность. Любой из 100 алгоритмов кластерного анализа, разобьет эти данные на 2 кластера. Проблема в том, что данные очень-очень разные в зависимости от разных встающих перед нами задач. И когда было всего 5 алгоритмов кластерного анализа, Иванов перепробовал их, но ему ни один не понравился.

<img src="/home/ekaterina/Desktop/AO/Лекция 10/20210507(10).jpg" style="zoom:15%;" />

Если Иванов бестолковый, то пойдет плакать в уголок, а когда у него будут спрашивать причину, он ответит: «кластерный анализ не работает на моих данных». Если он не бестолковый, то придумает 6 алгоритм кластерного анализа на его данных отработает, напишет статью, а Петров, Сидоров и другие прочитают эту статью и особенности их данных окажутся такими, как у Иванова, то они применят алгоритм Иванова и его включат в книжку к другим. Подобная история с методами классификации.

**2)**

**dp, max=1/n**, т.к. имеем начальное распределение с вероятностью выбора каждого признака **1/n**, то **> 1/n** не можем взять, т.к. отрицательных вероятностей у нас не бывает, поэтому max, который мы можем взять это **=1/n**.

Чем это плохо? Тем что можем убить хороший признак.

**dp, min=0**, долго-долго нет никакой адаптации, только случайный поиск. Т.к. мы не наказываем и не поощряем (отнимать или прибавлять 0, не имеет никакой полезности в информации), то адаптация исчезает, остается случайный поиск.

**3)**

**r, min=2**, попали оба плохих – несправедливо поощрю чуть менее плохого, попали оба хороших-несправедливо накажу чуть менее хорошего. Т.е. выбор хороших и плохих всего лишь по 2-м признакам:

<img src="/home/ekaterina/Desktop/AO/Лекция 10/20210507(11).jpg" style="zoom:20%;" />

вероятность попасть в хорошие или плохие – велика.

**r, max=нет!** Чем больше, тем больше вычислений.

**r, optimal=20**, принимаем на веру.

Например, взяли r=10

<img src="/home/ekaterina/Desktop/AO/Лекция 10/20210507(12).jpg" style="zoom:20%;" />

<img src="/home/ekaterina/Desktop/AO/Лекция 10/20210507(13).jpg" style="zoom:20%;" />

А вероятность, что они будут размазанные – высокая.

**4)**

Происходит адаптация. Чем больше **R**, тем лучше. Мы стремимся к наилучшей системе из **k** признаков.

Например, R=3000, т.е. должны провести 20*3000=60000 измерений.

Проходим по нашему **r*R**. Выбрали наилучшую систему из **k** признаков. 

SSA –sporadic search adaptation

```
best<-k SSA x1 x2       ⍝ x1 и x2 –матрицы для классов ω1 и ω2
```

Если **k** было взято 5 и **n** было 100, то **best** может выглядеть так **best: 11 93 5 44 59**, т.к. мы выбирали из 100 признаков случайно, то не обязана быть наилучшая группа упорядоченной.

Этот набор признаков нам обеспечит 90% точность и начинаем его применять в разных отраслях жизнедеятельности.

А чего, применяя SSA, полезного не сохранили как результат, кроме лучшего набора признаков. Что еще полезного после **R** шагов программы? Распределение вероятности выбора признаков (**p**)!!!

Если хотим получить лучший результат, придется всю работу начинать сначала. Но если в SSA засунуть уже выявленное лучшее распределение признаков, то будем выбирать уже из этого распределения и его будем поправлять и получится продолжение, а не начинание сначала.