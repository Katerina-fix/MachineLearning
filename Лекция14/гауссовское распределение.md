# Многомерный вероятностный метод классификации основанные на аппроксимации гауссовским распределением.



У нас есть **Nc**-число классов и для каждого класса мы оцениваем условную плотность распределения многомерную для векторочков **х**, при условии, что эти **х** принадлежат классу **⍵(i)**
Общая запись многомерного гауссовского распределения
![20210528_5](C:\Users\user\Desktop\Лекция14\20210528_5.jpg)
Почему это будет ковариационна матрица
Вот для двумерного случая, когда мы перемножаем эти две матрицы, то получаем 

<img src="C:\Users\user\Desktop\Лекция14\20210528_2.jpg" alt="20210528_2" style="zoom:15%;" />

￼<img src="C:\Users\user\Desktop\Лекция14\20210528_3.jpg" alt="20210528_3" style="zoom:15%;" />

Кто здесь **i** у нас этих векторов
￼<img src="C:\Users\user\Desktop\Лекция14\20210528_4.jpg" alt="20210528_4" style="zoom:15%;" />
Соответственно добавим индекс **i**
￼<img src="C:\Users\user\Desktop\Лекция14\20210528_9.jpg" alt="20210528_9" style="zoom:60%;" />
Далее

<img src="C:\Users\user\Desktop\Лекция14\20210528_6.jpg" alt="20210528_6" style="zoom:20%;" />

<img src="C:\Users\user\Desktop\Лекция14\20210528_7.jpg" alt="20210528_7" style="zoom:15%;" />

Коэффициент ковариации между 1 и 2 признаком. Т.е. наша матрица после суммирования и деления на  **n** выглядит следующим образом:

<img src="C:\Users\user\Desktop\Лекция14\20210528_8.jpg" alt="20210528_8" style="zoom:15%;" />

В многомерном случае, когда **n>2**

<img src="C:\Users\user\Desktop\Лекция14\20210528_10.jpg" alt="20210528_10" style="zoom:15%;" />

Т.к. ковариации не зависят от порядка признаков, то эта матрица симметричная.

Формула Байеса для условной вероятности того, что у нас наблюдается класс **⍵(i)** при условии, что померен вектор **x** - это произведение априорной вер-ти класса **⍵(i)** и на полную вероятность x при условии **⍵(i)** и делим на нормировочный множитель, который по всем классам пробегает.

<img src="C:\Users\user\Desktop\Лекция14\20210528_11.jpg" alt="20210528_11" style="zoom:15%;" />

Формула Байеса – гениальная вещь, потому что впрямую вероятность не можем оценить.

Оценить вероятность значит зафиксировать набор наших измерений, **х** -вектор из сахара в моче, верхнее давление и т.д., мы должны зафиксировать, что это равно тому-то и смотреть чем болен этот клиент. 

Например, болезнью **⍵1**, теперь ищем клиента с точно такими же анализами и посмотрим, чем болеет он, но мы никогда не найдем такого клиента, чтобы были анализы один в один с кем-то другим.

Собрав всех клиентов, которые болеют болезнью **⍵1** мы запросто можем оценить распределение показателей анализов, то же самое для **⍵2** и т.д.

Т.е. ту вероятность, которую мы не можем экспериментально получить, мы получаем благодаря Байесу.

Мы можем имея 3 класса посчитать для первого, второго и третьего классов формулу Байеса и выбрать тот диагноз, вероятность которого больше всего.

Но, чтобы это все не городить, т.к. мы выбираем максимум, переходим к решающим ф-циям, которые приведут нас к тому же решению, что и по полному распределению, но более простому в вычислительном плане.

Первое, что мы делаем при виде экспоненты – прологарифмируем.

![20210528_12](C:\Users\user\Desktop\Лекция14\20210528_12.jpg)

Для сравнения этих вероятностей один и то же знаменатель, поэтому он не важен, максимум останется прежним. Логарифмируем только числитель.

Общий случай, когда ковариационная матрица своя для каждого класса, тогда у нас функция будет решающая – квадратичная, т.е. наши измерения для какого-то объекта, который мы классифицируем, они будут входить во второй степени после перемножения этих матриц.

Если хотим один класс отличить от другого нам нужен **d1(х)** **d2(x)** и ищем максимум. Границей будет разность этих решающих функций приравненная к **0**.

![20210528_13](C:\Users\user\Desktop\Лекция14\20210528_13.jpg)

*Как упрощается при разных предположениях наша решающая функция?*

Одно из упрощений – принимаем, что ковариационная матрица одинакова для всех классов.

<img src="C:\Users\user\Desktop\Лекция14\20210528_14.jpg" alt="20210528_14" style="zoom:20%;" />

Ковариационная матрица в двумерном случае – это срез на какой-то высоте холмика распределения.

<img src="C:\Users\user\Desktop\Лекция14\20210528_15.jpg" alt="20210528_15" style="zoom:20%;" />

если мы договоримся о одном и том же ур-не среза, то с меньшими главными сторонами эллипс будет говорить, что для этого класса у нас менее сильная корреляция с такими признаками, а с большими главными сторонами, наоборот и наклон эллипса говорит, что для **⍵1** у нас сильна корреляция **х2** и **х1** и она положительная. для класса **⍵2** сильная корреляция, но отрицательная и для класса **⍵3** корреляция отсутствует, но дисперсия **х1** много больше, чем дисперсия **х2**.

Если мы предположим, что все **С** равны , то видим, что эллипсы отличаются только средним значением.

<img src="C:\Users\user\Desktop\Лекция14\20210528_16.jpg" alt="20210528_16" style="zoom:20%;" />

Направленность, которую указывает главная оси между **x1** и **x2** у всех одинаковая и размеры тоже.

В этом случае логарифм будет одинаков для всех **i**, т.к. ищем максимум, то определитель ковариационной матрицы мы выкидываем, а скобочки мы раскрываем, приводим подобные и в итоге получаем:

<img src="C:\Users\user\Desktop\Лекция14\20210528_17.jpg" alt="20210528_17" style="zoom:15%;" />

Где<img src="C:\Users\user\Desktop\Лекция14\20210528_18.jpg" alt="20210528_18" style="zoom:5%;" />  не включает **x**, это некая константа для каждого класса, а<img src="C:\Users\user\Desktop\Лекция14\20210528_19.jpg" alt="20210528_19" style="zoom:5%;" />  включает **x**, когда мы умножим обратную ковариационную матрицу на  мат.ожидание, то получим некий числовой вектор и когда мы умножим это вектор столбец на этот вектор строку, т.к. транспонированный, то получим сумму:

<img src="C:\Users\user\Desktop\Лекция14\20210528_20.jpg" alt="20210528_20" style="zoom:15%;" />

Это тоже некая константа.

Т.е. решающая функция линейно зависит от **x**, если возьмем разность для двух классов решающих функций и приравняем ее к нулю(к границе), то в данному случае получим выражение, которое показывает, что эта граница является линейной относительно **x**.

Поэтому, если имеем неравные ковариационные матрицы, то у нас она квадратичная относительно **x**.

<img src="C:\Users\user\Desktop\Лекция14\20210528_21.jpg" alt="20210528_21" style="zoom:15%;" />

А если ковариационные матрицы равны, то она будет для двумерного случая прямой, для трехмерного – плоскостью, для **n**-размерности – гиперплоскостью, но все это всего лишь линейная функция от всех **x**.

<img src="C:\Users\user\Desktop\Лекция14\20210528_22.jpg" alt="20210528_22" style="zoom:15%;" />

Следующий шаг по упрощению. это предположение, что наша ковариационные матрицы не только одинаковы, но и диагональные, т.е. ковариации **i** и **j =0**.

Запись для решающей ф-ции

<img src="C:\Users\user\Desktop\Лекция14\20210528_23.jpg" alt="20210528_23" style="zoom:15%;" />


Она означает, что у нас входит скалярное произведение вектора на мат.ожидание **i**-ого класса и диаметрически это будет

<img src="C:\Users\user\Desktop\Лекция14\20210528_24.jpg" alt="20210528_24" style="zoom:15%;" />

Получается случай эталонов.

Псмотрим как работает на примере диагностирования акустических шумов:

![20210528_25](C:\Users\user\Desktop\Лекция14\20210528_25.jpg)

*правый аргумент матрица спектров акустического шума

*левый аргумент, это вектор из 1 и 2, где 1-нормальный теплообмен, а 2-кризис теплообмена

Получили в результате обучения вектор размерности два, т.е. вектор векторов и  если посмотрим на размерность каждого элемента, то получим 4, от того, что нужно знать априорную вероятность, определитель ковариационной матрицы, мат.ожидание для каждого класса и обратную ковариационную матрицу.

Проводим классификацию этих спектров. Сравниваем по байесовскому многомерному правилу с физической классификацией, которую мы знаем(кризис или норма).

Для первого случая упрощения мы снова проводим обучение.

Для каждого класса после обучения мы имеем не 4, а 3 параметра, уже не нужен определитель ковариационной матрицы, потому что он у всех одинаковый.

Если эту классификацию сравним с истиной , то они не совпадают и когда мы считаем сколько у нас ошибок, то одна ошибка появляется от нашего предположения.

*Диагональная матрица*

К тем же данным ее применяем, по ним же проводим классификацию, классификация не совпадает с фактическим выражением.

#### KNN – k ближайших соседей

Он очень простой, индифферентный к размерности признаков, замечательный алгоритм.

![20210528_26](C:\Users\user\Desktop\Лекция14\20210528_26.jpg)

Не имеет значения сколько классов. **k**-параметр алгоритма. 

Пусть k=9, мы находим самого ближайшего соседа к точке с неизвестной классификацией, второго, третьего, четвертого,….,девятого ближайшего соседа и проводим круг с центром с точкой неизвестной принадлежности и радиус для k-ого ближайшего соседа.

Смотрим, что в круг попались два крестика, три треугольника и четыре кружочка. 

К какому классу отнести неизвестную точку? К тем, кого больше, т.е. к кружкам.

*Прелесть этого алгоритма*. Если трехмерное пространство признаков, нам все равно, мы опять для неизвестной точки находим расстояния до всех точек обучающей выборки, находи **k**-ую ближайшую и проводим через нее шар и считаем сколько элементов каждого класса вошло в него, далее по большинству эту точку классифицируем.

Если у нас 400-мерное пространство, ничего страшного.

<img src="C:\Users\user\Desktop\Лекция14\20210528_261.jpg" alt="20210528_261" style="zoom:15%;" />

Считаем расстояние от точки **x** до всех **y**, располагаем их в порядке возрастания, находим **k**-ую и для  <img src="C:\Users\user\Desktop\Лекция14\20210528_27.jpg" alt="20210528_27" style="zoom:5%;" />подсчитываем сколько элементов каждого класса и выбиваем по максимальному представительству.

Еще одна прелесть в том, что он работает не только для задач классификации, но и для регрессии.

Когда хотим в какой-то точке оценить **y**, если k=3, то ищем три ближайшие точки, применяем усреднение(медиана, взвешенное и т.п.)

<img src="C:\Users\user\Desktop\Лекция14\20210528_28.jpg" alt="20210528_28" style="zoom:15%;" />

В методе KNN регрессии, как и в KNN классификации все равно на размерность пространства. 

Трехмерное пространство:

<img src="C:\Users\user\Desktop\Лекция14\20210528_29.jpg" alt="20210528_29" style="zoom:15%;" />

Он тоже дает одну ошибку. Алгоритм **k** берется из оптимизации.

#### Метод потенциальных ф-ций.

Подразумевается разделение на 2 класса и он основан на том, что мы к каждой точке одного класса приписываем 1-ый положительный заряд, к каждой точке второго класса приписываем 1-ый отрицательный заряд и для неизвестной точке считаем какой потенциал, если он положительный, то относим к первому классу, если отрицательный – ко второму классу

![20210528_29](C:\Users\user\Desktop\Лекция14\20210528_29.png)

Для одномерного случая:

<img src="C:\Users\user\Desktop\Лекция14\20210528_30.jpg" alt="20210528_30" style="zoom:15%;" />