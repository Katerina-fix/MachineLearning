| МИНИСТЕРСТВО  ОБРАЗОВАНИЯ И НАУКИ РОССИЙСКОЙ ФЕДЕРАЦИИ   федеральное государственное АВТОНОМНОЕ  образовательное учреждение ВЫСШЕГО образования  «Национальный исследовательский  ядерный университет «МИФИ» |
| :----------------------------------------------------------: |
| **Обнинский  институт атомной энергетики –**   филиал  федерального государственного автономного образовательного учреждения  высшего   образования  «Национальный исследовательский ядерный университет «МИФИ»  **(ИАТЭ  НИЯУ МИФИ)** |

<center>Отделение интеллектуально-кибернетических систем</center>

 

 

 

 

 



 

 

### <center>Лекции по дисциплине "Обучение машин"</center>

 

 





 

 

<p align="justify">
    Выполнила: студентка гр. ИВТ-М20                                                                             Илюшкина  Е.Н.
</p>

<p align="justily">
    Проверил: профессор  ИАТЭ                                                                                       Cкоморохов  А.О.
</p>
















<center>Обнинск, 2021 г.</center>



# Содержание

##### 1. Что такое Machine Learning? Из чего оно состоит?.............................................................3

##### 2. Анализ данных (Data Science)................................................................................................6

##### 3. Возможности APL.....................................................................................................................11

##### 4. Построение графиков.............................................................................................................17

##### 5. Python vs APL...........................................................................................................................19

##### 6. График параллельных координат.........................................................................................23

##### 7. Метод Орлочи...........................................................................................................................40

##### 8. Метод гравных компонент (PCA)............................................................................................51

##### 9. Sammon.....................................................................................................................................62

##### 10. Методы, которые помогают оценить информативность разных признаков, это мы будем делать имея У...................................................................................................................73

##### 11. Многомерный критерий Фишера.........................................................................................83

##### 12. Случайный поиск с адаптацией..........................................................................................94

##### 13. Распознавание образов или классификация...................................................................101

##### 14. Алгоритм Кендалла на APL.................................................................................................105

##### 15. Многомерный вероятностный метод классификации основанные на аппроксимации гауссовским распределением..................................................................................................109



# Что такое Machine Learning? Из чего оно состоит?

1. #### Математическая статистика

   Пример: средний рост ВТ=ТД, если они одинаковы, то мы не получим ровно 169,3 см и там и там, получим отличающиеся цифры.

   Переходим к вер-тям:

![20210212_1](C:\Users\user\Desktop\Лекция1\20210212_1.jpg)

Это некая статистическая процедура при расказе о том какие данные получились и что с ними делать, это считалось приличным тоном в какой-нибудь статье.

Mathematical statistics - это раздел математики, который работает не с живыми или существующими объектами, а  с математическими моделями.

Со временем из-за появления новыхразных задач, слово mathematical ушло и осталось statistics.

2. #### Основное содержание(направление) науки.

   2.1 Экспериментальная парадигма – ничего кроме того, чтобы посмотреть сколько воды вытеснялось из ванны, когда лёг человек, если разными людьми этот эксперимент повторяется, то это хорошо для точности. И пытались описать какими-то зависимостями, но появляется 2.2.

   2.2 Теоретическая парадигма – главное, что часто эксперимент оставался на первом месте, а теория приходила объяснять его, но очень часто они менялись местами из чего получался результат, но правильный он или нет – неизвестно, следовательно, результат требовал экспериментального подтверждения. 

   2.3 К 80-м годам и после, сильно начала развиваться вычислительная техника. Появилась возможность разработки очень сложных математических моделей, получили возможность ставить эксперимент не на реальном объекте, а на его математической модели. Вычислительная (математическое моделирование) 

   2.4 Data Science – например, если разные данные анализов пациентов и проанализировав их можно уже предварительно, по закономерностям предположить диагноз.

   После обозначения Statistics, стали употреблять название Data Analyses (анализ данных). Она показывает как без калькулятора можно проводить анализ. 

3. #### Knowledge Discovery Databases - открытие/выявление знаний в БД. 

   Знания – это что-то не очевидное для нас, какие-то зависимости, которые будут представлять интерес и которые мы можем использовать. 

   Например: лейкоцитов в крови много и сахара в матче тоже, следовательно, какая-то болезнь.

4. #### Data Maining (maining - шахтерское дело)

   Добыча данные, данные не имеют смысла, только результат их анализа.

5. #### Machine Learning 

   Обучение машин, мы хотим научить машину решать сложные интеллектуальные задачи, анализировать изображения, заниматься медицинской диагностикой и т.д.

6. #### Data Science 

   Наука о данных.

**Х**(m*n) – матрица объект(строки)/свойства(столбцы). 

<img src="C:\Users\user\Desktop\Лекция1\20210212_2.jpg" alt="20210212_2" style="zoom:10%;" />

Объект – запись состояния реактора в какой-то момент времени или состояния одного из ГЦНов, больной Сидоров и так далее.

Свойства – с помощью чего описывается объект (температуры давления в стоимость и так далее). 

У нас может быть/не быть вектор **У** размером  m*1:

<img src="C:\Users\user\Desktop\Лекция1\20210212_3.jpg" alt="20210212_3" style="zoom:10%;" />

"у" может быть записан как номинальная или непрерывное шкала. 

*Номинальная* – наименования, то есть нельзя складывать/вычитать, можно написать какой болезнью болеет наш k-ый клиент и тогда на k-ом месте будет имя его болезни (у меня это место под цифрой 2).

*Непрерывное* – например, температура и тому подобное. 

В зависимости от того, какой **У** мы решаем две задачи: 

1. *У* непрерывный – регрессионный анализ (МНК, нейронные сети), он ищет такую функцию f(x), который предсказывает с высокой точностью *У*
2. *У* номинальный классификации (распознавания), снова строим У=f(x), которая позволяет для заданного набора свойств предсказать *У* (например, номер болезни).

Если у нас есть только **Х**, то проводим кластерный анализ. 

Кластера называется набор точек близких между собой и отличающихся от точек других классов.

Синоним кластера – группа. 

<img src="C:\Users\user\Desktop\Лекция1\20210212_4.jpg" alt="20210212_4" style="zoom:15%;" />

Если получили такую картинку, то радуемся. 

В матрице **Х** есть три разных сорта, отличающихся друг от друга. 

Если спроецируем данные на X_5, то получим только две группы, а по Х_13 - только одну. 

#### Первый вопрос, который мы должны задать: 

ЧТО ЭТО ЗА ГРУППЫ??? ЧТО КУДА ПОПАЛО И ПОЧЕМУ?!

# Анализ данных (Data Science) #

![](C:\Users\user\Desktop\Лекция2\Рис.1.png)

1. ***Данные*** - взяли пачку мед карт каких-то больных и списали из этой пачки лейкоциты в крови, сахар в моче, верхнее/нижнее давление, пульс и т.п., получился набор данных.
   Теперь с этими данными нужно что-то делать.

2. ***Математика*** - в этой математике нет каких-то теорем, лемм и пр., нельзя ничего заранее доказать не попробовав. Хотя под математикой в частности мы помечаем: лин.алгебра(техника работы с матрицами и векторами); теория вероятности(без всяких закидонов, достаточно понимать, какова частота наблюдать некое отклонение от среднего и т.д.) и прочие базовые направления.

   Выше матиматики следует упомянуть алгоритмы. Алгоритм - это последовательность действий, возможно содержащие некие условия, которая должна быть выполнена, чтобы получить результат который нас интересует. А результатом будет при отсутствии Y в наших данных - кластерный анализ.

   Важно! Что алгоритмы часто являются ЭВРИСТИЧЕСКИМИ, т.е. за ними не стоит математика каких-то теорем и т.д.. Например, какой-то Пупкин подумал: "Что если я для выделения групп буду использовать такую процедуру"

   ![Рис.2](C:\Users\user\Desktop\Лекция2\Рис.2.png)

   Пупкин захотел вычислить цепные расстояния. Расстояние между двумя точками на плоскости - это разность одноименных координат:

   ![Рис.3](C:\Users\user\Desktop\Лекция2\Рис.3.png)

   Расстояние в многомерном пространстве ничем не отличается: 
   $$
   r=sqrt(a^2+b^2+c^2......)
   $$
   График цепных расстояний это значит, расстояние между первой и второй, между второй и третьей и т.д. точками.

   ![Рис.4](C:\Users\user\Desktop\Лекция2\Рис.4.png)

   Максимум получился между третьей и четвертой, пятой и шестой точками.

   Выделям кластеры: С1, С2, С3

   Но пики могут быть не всегда, расстояние будет болтаться на каком-то одном уровне.

   ![Рис.5](C:\Users\user\Desktop\Лекция2\Рис.5.png)

   Этот алгоритм прост, но очень ущербен тем, что в общем случае не работает нумерация точек. Он может работать только при наличии гипотезы о том, что если есть разделение по кластерам, то она происходит упорядоченно с номером точки, например, при изучении кризиса теплообмена(на поверхности тепловыделяющего элемента образуется паровая пленочка, это приводит к резкому температурному скачку твэлла, т.е. расплавлению и выходу продуктов деления в теплоноситель). Делался эксперимент:

   ![Рис.6](C:\Users\user\Desktop\Лекция2\Рис.6.png)

   Увеличивая мощность, на каждой мощности получали аккустический спектор, т.е. в зависимости от частоты и амплитуды, мы видим, что где-то наступил кризис, это мы знаем из контроля температуры стенки твэлла.

   Здесь метод цепных расстояний годится, если кластеры нормального теплообмена и кризис теплообмена существуют, то они должны естественно по мощности быть разделены, т.е. не может быть, чтоб первый спектор, который при маленькой мощности мерился - это кризис, а последний, котрый при большой - это норма. НЕТ! Может быть только норма, норма, норма, потом при увеличении мощности кризис, кризис, кризис и нормы уже быть не может.

   !ВОПРОС! Почему мы берем какую-то книжку по Data Science, обязательно будет раздел кластерного анализа. Мы удивимся, что будет очень много методов. И будет очень много книг с подобным разделом и везде чертова туча алгоритмов кластерного анализа. ПОЧЕМУ ИХ ТАК МНОГО???

   ЕСЛИ ЖЕНЩИН МНОГО, ТО ЗНАЧИТ НЕТ ОДНОЙ. Следовательно, если алгоритмов много, то значит нет одного, который решал бы с любыми данными задачу кластерного анализа. Алгоритмов много, потому что они зависят от ситуации и характеристик данных, которые мы имеем.

   ##### Big Data - Большие??? данные. 

   То что сегодня считается большим, завтра станет средним, после завтра - маленьким. Получился этот термин не по объему данных или скорости, а потому что данные с большого числа источников поступали непрерывно и их нужно было обрабатывать в темпе поступления, не сохраняя на диск (кодировка торговых данных, валют и т.п.)

   На каких-то искусственных примерчиках, мы можем ручками что-то набрать. Но когда большой объем данных, ничего не возможно без компьютера, а конктретно - программы.

3. ***Программа.*** 

   Программу будем писать на самых популярных языках: Python и R.

   Они хороши своей относительной простотой в том плане, что здесь не нужно быть гиков, чтобы их использовать для анализа данных. Самое главное, что в них есть бесконечное множество библиотек, а библиотека - это целое множество программ для анализа данных. Это языки с открытым исходным кодом.

   Никто бы не использовал Python без расширения NumPy, который содержит мощь и красоту работы с массивами данных. 

   R в отличии от Python - язык не общего назначения, на нем только анализируют данные.

   На вопрос, что лучше ставить? Ответ: BOTH!!! (Оба)

   #### APL

   A(неопределенный артикль) Programming Language - какой-то язык программирования. Создал его велики программист и матиматик Кеннет Айверсон.

   ![Кеннет Айверсон](C:\Users\user\Desktop\Лекция2\Кеннет Айверсон.jpg)

   В окружающем мире существует много нотаций - какие-то принятые соглашения по поводу того, что тот или иной знак означает. Если бы все трактовалось буквами, было бы, например, много автомобильных аварий или электросхемы нужно было бы расписывать на кучи ватманоских листах вместо одного.

   Или в арифметике, у нас есть пример 1+1=2, если бы не было нотации, мы бы писали: один плюс один равно два. УЖАС УЖАСНЫЙ!

   Кеннет Айверсон. Он преподавал в университете преобразование информации для анализа данных. Устав стучать мелом по доске и долго-долго объяснять какие-то простые алгоритмы преобразования данных, психонул, сел и придумал краткую, выразительную математическую нотацию, которая позволяет любые преобразования данных компактно и наглядно записывать. 

   Он описал эту нотацию в книге, которая вышла в 1962 году под названием "A Programming Landuage". Самое смешное то, что Кеннет Айверсон, создавая эту нотацию и выпуская эту книгу,  не думал о компьютерах, не думал, что APL будет языком программирования, т.к. использовал программирование в другом смысле как передча или язык для записи алгоритмов. Эта книга имела взрывной успех по всему миру. Сразу появились книги: "Математическая статистика на APL", "Линейная алгебра на APL", где уже какие-то специализированные области математики и чего-то еще записывали именно на APL и показывали как это удобно, хорошо, коротко получается.

   В 1966 году IBM пригласила Айверсона и еще несколько человек разработать на базе APL язык программирования, который на машинах IBM работал бы.

   Это настолько знаменательное событие, что Кеннет Айверсон получил премию Тьюринга в 1979 году, которая приравнивается к премии Нобиля. А в 1982 году - премию компьютерного пионера как первопроходец. Они считаются как две самые великие премии, которые может получить программист или математик.

   Но пока нас это не убеждает в том, что APL нам нужен для анализа данных.

# Возможности APL

У АПЛ огромное число уникальных св-в и преимуществ. Мы поговорим о основных, т.к. их оооочень много.

Одно из свойств то, что Айверсон, те математические общепринятые операции с обозначениями, оставил так как они есть.

Например:

```
      3+2
5
      3×2
6
      3÷2
1.5
```

Видим, что те математические операци, которые общеприняты, так и остаются общепринятыми.

Чтобы не морочиться со словами, логически False = 0, True = 1

```
      3<2   
0
      3>2
1
```

Все общепризнанные знаки сохраняются, например, в теории множеств есть ∊ - принадлежность

Проверим, принадлежит ли 2-ка множеству, которое в правом аргументе.

```
      2∊7 6 2 4
1
```

А 12 - будет нуль.

```
      12∊7 6 2 4
0
```

Т.е. нам не нужно заморачиваться со словами заранее определенными, большинство операций совпадают с общепринятыми в математике.

***Теперь важнейшая особенность***

```
      3+2 1 5     ⍝ вектор, в котором к каждому эелементу прибавится по тройке
5 4 8
      1 2 3+2 1 5 ⍝ сложение двух векторов
3 3 8
```

Т.е. важное достоинством АПЛ то, что он является ориентированным на работу с массивами любой размерности как с целым.

Нам не надо писать никакие несчастные циклы, мы прямо даем операции в качестве аргументов или два скаляра, или два вектора, или скаляр и вектор и автоматически выполняется или поэлементно эта операция, или, если один из аргументов скаляр, то поэлементно с этим скаляром все выполняется.

Делаем матрицу А и В

```
      A←?3 4⍴9
      B←?3 4⍴9
```

Получили две матрицы 3 на 4

```
      A B
 9 8 2 9  3 3 8 6 
 6 9 7 3  8 8 1 6 
 5 5 6 8  2 5 1 4 
```

```
      А + В ⍝ сложение матриц по элементам, получим одну матрицу 3 на 4
12 11 10 15
14 17  8  9
 7 10  7 12
      A - B ⍝ поэлементное вычисление матриц
 6 5 ¯6  3
¯2 1  6 ¯3
 3 0  5  4
```

Нам плевать, что такое А и В - это два скаляра, два вектора, матрицы, два массива каких-то пятимерных, мы выполним эту операцию без всяких циклов и заморачивания.

**Внимание на минус вверху число**, потому что это знак числа, чтобы мы его не путали с функцией.

**Выполнение в АПЛ идет справа налево.** Некоторые чудаки говорят: "Что это такое? Как в Китае".

![](C:\Users\user\Desktop\Лекция 3\20210226.png)

Первым делом вычислится корень, потом косинус от получившегося числа, а потом уже синус.

Еще один пример на APL:

```
      2+3 4 - 2 1
3 5
```

Проходим 1-ку и 2-ку, никаких ф-ций нет, образовался массив  !2 1!. Далее функция "-" (откда вычитать?), идем левее функции, видим 4 и если бы у нас дальше ничего не было, мы бы из 4 отняли 2 и из 4 вычли 1, но мы идем дальше и видим, что это вектор {3;4}. Теперь выполняем первую операцию, от вектора {3;4} вычитаем {2;1}, получится {1;3}. Теперь функция "+" (к чему?), торчит дваечка. левее ничего нет, значит к 2 прибавляем 1 и к 2 прибавляем 3, получаем {3;5}.

*Нормальное течение исполнения может быть нарушено круглыми скобками.*

Так мы не нарушаем, а облегчаем восприятие:

```
      2+(3 4 - 2 1)
3 5
```

А так нарушаем, получается, что справа вектор {3;4;-2;1} и поэлементно прибавляем 2-ку:

```
      2+3 4 (- 2) 1
3 5
```

Стрелочка влево - знак присваивания, почему не равно? Потому что "=" - это логическая ф-ция, как "<", ">", и т.п.

Иначе:

```
      9 5 1 = 5 ⍝ получаем 9 не равно 5, поэтому 0, 5 равно 5, поэтому 1 и т.д.
0 1 0
```

поэтому присваиваем в Х.

```
      ?9 ⍝ черт знает что, от единицы до девяти, ? - генератор случайных чисел
2
      ?9
4
```

```
      x←?9 9 9 9 ⍝ четыре случайных числа, целых, от единицы до девяти
      x
5 1 3 4
```

Как сложить все х? Если у нас тысячи измерений, складывать вручную "+" - не вариант. Для этого потребуется "/" - оператор редукция. *f/x* - это:
$$
х_1f x_2f .....x_nf
$$
Т.е. та функция, которая слева от оператора, помещается в уме между всеми элементами аргумента и после происходит выполнение.

```
      +/х
13
```

***Прелесть в том, что с этой редукцией мы можем посылать любую функцию.***

```
      ×/х  ⍝ получим вроизведение всех элементов х
60
```

Очень хорошим свойством АПЛ является то, что все переменные  носят с собой "набор документов", которые можно спросить и получить информацию о ней.

Например, фунция ⍴ - размерность, вернет число элементов, если речь идет о векторе

```
      ⍴x
4
```

Если будет матрица, то получим вектор {3;4}, первое - число строк, второе - число столбцов

```
      ⍴A
3 4
```

Чтобы узнать число элементов, то нужно применить " ×/"

```
      ×/⍴A
12
```

#### **!!!САМЫЙ ПИСК!!!**

![](C:\Users\user\Desktop\Лекция 3\20210226(1).png)

Записали среднее как в учебниках математики и как оно будет выглядеть в АПЛ. Получили однуэтажную, очень компактную, математическую функцию.

А дальше СЁКС пошел. Студенты переписали в тетрадь эту строку. Пришли на лабораторные работы и записали ту же строку в АПЛ и нажали Enter. Получили среднее арифметическое вектора х

```
      +/x÷⍴x
3.25
```

Т.е. АПЛ - это не только сверхкомпактная, математически понятная, без специальных программистских словечек математическая нотация, а это ***ИСПОЛНЯЕМАЯ МАТЕМАТИЧЕСКАЯ НОТАЦИЯ!!!*** Нет границы между записью алгоритма и программой, нет границы между программой и алгоритмом. Это выдающееся и нигде больше неприсутствующееся свойство АПЛ. 

*Замечание!* В АПЛ кроме встроенных или примитивных ф-ций можно определять какие-то свои. В АПЛ нет собственных векторов или значений, если они нам приспичили, можем сделать такую ф-цию сами.

Например:

```
      )ed ave
```

![](C:\Users\user\Desktop\Лекция 3\20210226(2).jpg)

аргументом будет х - вектор, а результатом будет у, и строку со средним присвоим в y. 

```
      ave x*2 ⍝ среднее квадратов х
12.75             
```

Большинство ф-ций АПЛ могут иметь или 1 аргумент, тогда говорят монадик, например, ⍴x - одномерное использование ф-ции, или 2 аргумента - доядик (9⍴2 - двуместное использование, сделать вектор длиной 9 из двоек)

```
      ⍴x
4
      9⍴2
2 2 2 2 2 2 2 2 2
```



##### Рассмотри как себя ведет подбрасывание монеток

```
      х←?9⍴2 ⍝ 9 случайных чисел от 1-2, т.е. будет вектор из девяти элементов принимающие значения 1 или 2
1 2 2 2 1 1 1 2 2
```

Герб = 2, тогда в серии из 9 подбрасываний присваиваем 2-ку, получаем 1, где у нас выпала 2, и 0 - где выпала единица

```
      2=x
0 1 1 1 0 0 0 1 1
```

 Если проссумируем, то получим сколько раз у нас выпал Герб

```
      +/2=x
5
```

 Если разделим на кол-во раз, сколько подбрасывали монетку, сделаем это так

```
      (+/2=?n⍴2)÷n←9
0.5555555556
      (+/2=?n⍴2)÷n←99
0.5353535354
      (+/2=?n⍴2)÷n←99
0.4848484848
      (+/2=?n⍴2)÷n←999
0.4734734735
      (+/2=?n⍴2)÷n←999
0.5055055055
      (+/2=?n⍴2)÷n←999
0.4834834835
      (+/2=?n⍴2)÷n←9999999
0.50019955
      (+/2=?n⍴2)÷n←9999999
0.50002765
      (+/2=?n⍴2)÷n←9999999
0.50012905
```

присвоили число раз на которое мы подбрасываю а потом поделим на него сумму, то получим ВЕРОЯТНОСТЬ с которой у нас выпадет Герб за эти 9 бросков.

В машинном обучении мы будем использовать АПЛ для демонстрации каких-то алгоритмов обучения, как они и на каких данных работают.

Очень сложной ф-цией является та, которая имеет 5-7 строчек, реализация очень сложного алгоритма, но нам это не грозит

Есть в машинном обучении метод группового учета аргументов, который называют иногда полиминиальными нейронными сетями. На бейсике программа занимала огромное число листов А4 (185см+вытянутая рука и стул и все равно листы лежали на полу), а на АПЛ гватило половинки листа А4.

***ML = 3 - мигрейшен левел, системная переменная, для описания системы***

IBM ввела обобщенные массивы

```
      ⍴(2 3)(2 4 5)(1 2 6 3 2) ⍝ вектор из 3-х элементов, где элементами являются векторы
3
```

```
      ⍴¨(2 3)(2 4 5)(1 2 6 3 2) ⍝ чтобы узнать размерность каждого элемента в преддущем векторе используем оператор "¨" (ич) - по каждому
2 3 5
```

```
      +/¨(2 3)(2 4 5)(1 2 6 3 2) ⍝ получим сумму элементов элементов вектора
5 11 14
```

```
      ⍴A B ⍝ вектор длины два
2
      ⍴¨A B ⍝ каждый элемент вектора является матрицей 3 на 4
 3 4  3 4 
      ]disp A B ⍝ утилитка ]disp графически показывает как устроен обобщенный массив
┌→──────┬───────┐
│9 8 2 9│3 3 8 6│
│6 9 7 3│8 8 1 6│
│5 5 6 8↓2 5 1 4↓
└~─────→┴~─────→┘
```

Обощенные массивы резко повышают возможность реализации любых алгоритмов АПЛ.

# Построение графиков

Подгружаем пакет plt

```
      ]load plt
#.plt
```

Строим какой-то х точками

```
      x
2 4 1 5 7      
      plt.plot x
```

![](C:\Users\user\Desktop\Лекция 3\20210226_1.bmp)

Строим х линиями

```
      1plt.plot x
```

![](C:\Users\user\Desktop\Лекция 3\20210226.bmp)

Чтобы не набирать постоянно plt.plot:

```
      ⎕path←'plt' ⍝ path-список мест, где при поиске файла можно не задавать директорию, где этот файл лежит
```

 Можем теперь писать:

```
      plot x
```

![](C:\Users\user\Desktop\Лекция 3\20210226_1.bmp)

# Python vs APL

```
      Python:                                                 APL:
      x=arange(1,5)                                          x←⍳4
      x
array([1,2,3,4])
      sum(x)                                                 +/x
10
      add(x,x)                                               x+x
array([2,4,6,8])
      add.reduce(x)   ⍝'reduce'-в apl расставляет            +/x
      ф-цию (add 1(элемент) add 2 add 3
```

Импорт (from numpy import*)

```
      m=add.outer([2,1]x)    ⍝'outer'-внешнее произведение   2 1 ∘.+x
      m                                                       m
array([[3,4,5,6],[2,3,4,5]])                                  3 4 5 6
                                                              2 3 4 5
      sum(m)                                                  +/m
32
      add.reduce(m)                                          +/[1]m  ⍝+⌿m
array([5,7,9,11])
      add/reduce(m,axis=1)   ⍝'axis'-ось                     +/m  ⍝по строкам
array([18,14])
      multiply.reduce(m,axis=1)                               ×/m  ⍝по строкам
array([360,120])
      shape(m)                                                ⍴m
(2,4)
      m.shape
(2,4)
      m1=m                                                    m1←4 2 ⍴m 
      m1.shape=[4,2]   ⍝транспанирование
      m1
array([[3,4],
       [5,6],
       [2,3],
       [4,5]])
      vstack((m,m))   ⍝конкретинируем по одному измерению    m,[1]m
array([[3 4 5 6],
       [2 3 4 5],
       [3 4 5 6],
       [2 3 4 5]])
      mm=vstack((m,m))                                        mm←m,[1]m
      mm.shape                                                ⍴mm
(4,4)
      mm=hstack((m,m))                                        mm←m,m
      mm
array([[3,4,5,6,3,4,5,6],                                     3 4 5 6 3 4 5 6
       [2,3,4,5,2,3,4,5]])                                    2 3 4 5 2 3 4 5
      mm.shape                                                ⍴mm
(2,8)
      x
array([[1 2 3 4 ]])
```

Перемножение по правилу линейной алгебры

```
      x
1 2 3 4
      m1
3 4
5 6
2 3
4 5
      dot(x,m1)                                               x+.×m1
35 45
```

from numpy import *

```
def med3(x):
     y = vstack((x[:-2],x[1:-1],x[2:]))
     y = y.sum(axis=0)-(y.min(axis=0)+y.max(axis=0))
     return hstack((x[0],y,x[-1]))

def hann(x):
     y = 0.25*x[:-2]+0.5*x[1:-1]+0.25*x[2:]
     return hstack((x[0],y,x[-1]))
```

На APL:

**Сглаживание выборки** - это преобразование значений выборки таким образом, чтобы
уменьшить их разброс относительно соседних с целью выявления основной тенденции их
поведения.

*Крайние значения, не подвергшиеся обработке, заменяются ближайшим преобразованным значением.*

**Ганнирование** - это сглаживание путём замены каждого значения выборки на взвешенное
среднее между ним и двумя соседними, причём вес при основном значении в 2 раза больше
веса соседних.

![20181109](C:\Users\user\Desktop\Лекция 4\20210305.png)

![20210305(C:\Users\user\Desktop\Лекция 4\20210305(1).png)](C:\Users\user\Desktop\Лекция 4\20210305(1).png)

**Формула в APL:**

```
hann←{1⌽x[⌽1,⍴x],x←.25 .5 .25+.×⍉⊃3,/⍵}
```

Примеры сглаживания ганнированием:

Мы создали синусоиду и зашумили её:

```
]load stat
#.stat
      ]load plt
#.plt
      ⎕path← 'plt stat'
t←0,.1×⍳200
s←1○0,.1×⍳200
sd←s+.3×stat.rnd ⍴s
sd←sd-0.15
1 0 1 plot t s sd (hann sd)
```

![20210305(C:\Users\user\Desktop\Лекция 4\20210305(2).BMP)](C:\Users\user\Desktop\Лекция 4\20210305(2).BMP)

#### Сглаживание медианой по тройкам

1. Мы берём 3 последовательно расположенных значения выборки.
2. Находим медиану (это будет одно из этих значений).
3. Заменяем центральное значение этой медианой.

***Формула в APL:***

```
med3←{(+/m)-(⌈/m)+⌊/m←⊃3,/⍵}
1 0 1 plot t s sd (med3 sd)
```

![20210305(C:\Users\user\Desktop\Лекция 4\20210305(3).BMP)](C:\Users\user\Desktop\Лекция 4\20210305(3).BMP)

**Замечание.** В случае наличия выброса при ганнировании искажаются точки, соседние с
выбросом, в то время как медиана по тройкам просто отбрасывает его.

# График параллельных координат 

Пусть будет 5 признаков:

x_1, x_2, ...., x_5

Каждый из них может состоять из скольки угодно элементов

![1](C:\Users\user\Desktop\Лекция 5\1.png)

Тут у нас m измерений, чему оно равно нас не волнует

1.Отнормируем каждый признак на интервал [0;1]

2.Отнимим минимальное значение от каждого и поделим на размах

3.Первое, что мы делаем, это отнимаем минимумы. Минимум становится равным 0, а вот максимум = макс-мин

4.Когда поделим на размах все эти точки (то на прямой появится единица=макс )

![2](C:\Users\user\Desktop\Лекция 5\2.png)

#### Как строится график после нормировки?

Очень просто. По оси абсцис - номер или имя признака, по оси ординат - нормированные значения от 0 до 1

![3](C:\Users\user\Desktop\Лекция 5\3.png)

И вот мы берем первый объект и у него х_1=0.5, х_2=0.8, х_3=масенький, х_4 и Х_5 где-то равны. Соединяем эти точки прямыми.

Переходим ко второму объекту. Снова наносим все точки и соединяем их. 

Делаем так, пока все объекты мы не описали.

Реализация на APL:

Соединяем все ириски в матрицу ir и проверяем размерность. У нас 150 цветочков и 4 признака

```
      ⍴ir←ir1⍪ir2⍪ir3
150 4
```

Получаем график ломаных

```
      (150/1) parcoor ir
```



![10](C:\Users\user\Desktop\Лекция 5\10.BMP)

Что гляда на график можно сказать? Если наклонить головку вправо и представить как будет выглядеть гистограмма, она будет близка к равномерному распределению. От минимума до максимума мы видим, что все 150 цветочков перемешаны в кучу и нет никакой структуры соответственно. (Это по первому признаку)

Глядя на второй признак, видим, что все цветки снова перемешаны.

А вот глядя на третий признак. ОПА!!! Видим две группы. Часть цветков имеют малое значение этого признака, а другая часть - большие значения этого признака.

Аналогично по признаку 4.

Выбираем интересные на наш взгляд признаки 3 и 4. Построим для них графичек. 

```
plot ir[;3 4]
```

![20181207(C:\Users\user\Desktop\Лекция 5\11.BMP)](C:\Users\user\Desktop\Лекция 5\11.BMP)

Видем две группы, два кластера. Мы открыли то, чего не ожидали, что все цветки разделены на 2 группы. Мы не ожидали, что все цветки разобьются на две группы.

Теперь мы рассматриваем эти две кучки, Зовем Марью Ивановну и она нам говорит. Что одна группа принадлежит одному сорту, а во второй группе у нас смешаны два сорта. И Марья Ивановна нам их рисует. Но перед этим мы построим покрашенный график параллельных координат.

```
      (50/⍳3) parcoor ir
```

![12](C:\Users\user\Desktop\Лекция 5\12.BMP)



По первому признаку черный, красный, синий сорты перемешаны к чертовой матери.

По второму признаку анлогично.

А по третьему и четвертому. Черненький (первый сорт) сильно отстоит от синего и красного сортов. Есть возможность поставить порог.

Мы видим, что не смотря на то, что чуть-чуть они перемешаны (4 красненьких попали к синеньким), т.е. мы не можем безошибочно разделить эти два класса, но можно сказать, что синенькие имеею меньшие значения признаков, чем красные.

Теперь можем постоить плоскость 3 и 4 признака и покрасить.

```
      (50/⍳3)plotc ⊂[1]ir[;3 4]
```

![13](C:\Users\user\Desktop\Лекция 5\13.BMP)

Этот образ отличается от первого, тем, что в принципе этому графику наплевать, сколько у нас признаков 4 или 400. Это очень мощный метод визуализации.

Еще один метод на котором мы подробно не будет останавливаться. 

#### Radar plot ( у него много названий)

Продемонстрируем в Экселе. Это тот же самый графичек, но который мы строим в полярных координатах.

![14](C:\Users\user\Desktop\Лекция 5\14.png)

Видим, как у нас отличаются эти два столбца по своим значениям. И если их много, мы можем тоже выявить какие-то разные типы. Часть с острыми уголками, часть без острых уголков. 

Вот это мы можем положить картинки и сказать: "Дети, разложите пожалуйста картинки на две кучки" и они прекрасно проведут это кластерный аналих.

Все эти графики позволяют выявить какие-то интересные признаки и понизить размерность пространства, т.е. из большого числа Х выбрать небольшое и в небольшой размерности мы можем замечательно смотреть. 

#### Теперь посмотрим простые два примера понижения размерности пространства:

1. ***Расстояние до двух фиксированных точек.*** это центры классов чаще всего фиксированных точек. Если известно, что есть два класса омега1 и 2, то две фиксированные точки. Так это будет выглядеть

![5](C:\Users\user\Desktop\Лекция 5\5.png)

М1-центр первого класса

М2-центр второго класса, строим следующий график

![4](C:\Users\user\Desktop\Лекция 5\4.png)

по оси абсцис расстояние до R1, по ординат - до R2

Сначала идут омега 1, потом омега два

У омега1 расстояние до М1 маленькое, а до М2 - большое. Для омега2 наоборот.

У нас изменются деления по осям, а вид графика незначительно изменится.

**А зачем мы его строим?** Его не надо было строить в двумерном пространстве потому что и так все видим, а вот в многомерном пространстве нам его надо построить.

***Начнем с двумерного:*** 

```
      a←?9 2⍴0     ⍝ А-первый класс(9 случайных точек от 0 до 1)
      b←3+?7 2⍴0   ⍝ В-второй класс(7 случайных точек от 0 до 1) и мы прибавим 3-ку
```

Строим:

```
      plot (⊂[1]a)(⊂[1]b)
```

![1](C:\Users\user\Desktop\Лекция 5\1.BMP)

Если вычислим М1. Напишем ф-цию ave и она будет суммировать матрицу по первому измерению и делить на число строк. 

```
      ave←{(+⌿⍵)÷≢⍵}
```

Посчитали центры

```
      m1←ave a
      m2←ave b
```

Теперь мы можем их пометить на графике

```
      color 'red'
      marker m1  ⍝ центр первого класса
      marker m2  ⍝ центр второго класса
```

![1](C:\Users\user\Desktop\Лекция 5\1.PNG)

Теперь слепим а и b в одну матрицу, чтоб не мучиться и R1 - это растояние от М1 до всех точек. R2 аналогично.

```
      ab←a⍪b
      r1←(+/(ab-[2]m1)*2)*0.5
      r2←(+/(ab-[2]m2)*2)*0.5
```

Построим

```
      plot r1 r2
```

![2](C:\Users\user\Desktop\Лекция 5\2.BMP)

Распределение перевернулось, но ничего равным счетом не поменялось

Покрасим:

```
      (9 7/1 2)plotc r1 r2
```

![9 7_1 2](C:\Users\user\Desktop\Лекция 5\9 7_1 2.BMP)

Когда у нас и так все было видно, то не имело смысла, что-то городить.
Тогда представим, что теперь у нас не двумерное, а ***стомерное пространство***!

```
      a←?9 100⍴0
      b←3+?7 100⍴0
      ⍴a
9 100
      ⍴b
7 100
```

Никакого графика мы здесь не нарисуем, можем сделать параллельные координаты. Всего у нас 16 точек.

```
      (16/1)parcoor a⍪b
```

![4](C:\Users\user\Desktop\Лекция 5\4.BMP)

Здесь трудно понять, что к чему относится.

Считаем опять М1 и М2 - центры:

```
      m1←ave a
      m2←ave b
      ⍴m1
100
      ⍴m2
100
```

Они у нас теперь 100-мерные.

Делаем а и b. Считаем растояние первого центра до всех точек

```
      ab←a⍪b
      r1←(+/(ab-[2]m1)*2)*0.5
      r2←(+/(ab-[2]m2)*2)*0.5
```

И стоим графичек. Видим две группы на плоскости.

```
      plot r1 r2
```

![5](C:\Users\user\Desktop\Лекция 5\5.BMP)

Можем взять 1 признак (кто меньше/больше 15)

```
      +/r1<15
9
      +/r1>15
7
```

Важно, что мы были в 100мерном пространстве и нифига не увидели, зная что первые точки относятся к одному классы, а другие ко второмы. Посчитали расстояния до центров от всех точек.

```
      (9 7/1 2)plotc r1 r2
```

![7](C:\Users\user\Desktop\Лекция 5\7.BMP)

Чтобы по 100 раз не набирать, сделаем ф-цию:

```
)ed m12proj
      
[0]    r←a m12proj b;ab;m1;m2;r1;r2  ⍝ аргументы матрицы a и b. Результат - вектор                                          векторов расстояний.
[1]    m1←{(+⌿⍵)÷≢⍵}a                ⍝ считаем среднее по a      
[2]    m2←{(+⌿⍵)÷≢⍵}b                ⍝ считаем среднее по b 
[3]    ab←a⍪b                        ⍝  делаем матрицу a и b, где у нас слепятся обе
[4]    r1←(+/(ab-[2]m1)*2)*0.5       ⍝  посчитаем расстояние
[5]    r2←(+/(ab-[2]m2)*2)*0.5     
[6]    r←r1 r2                       ⍝ в результат передадим r1 и r2
     ∇                   
```

Зелененьки-переменные глобальные(они нужны только на момент исполнения ф-ции), когда записываем в [0] они становятся локальными и стали беленькими.

Постоим:

```
      plot a m12proj b
```

![8](C:\Users\user\Desktop\Лекция 5\8.BMP)

На выходе у нас будет вектор расстояний до первого класса и вектор расстояний до второго класса.

2. ***Если у нас тот случай, который очень часто встречается. Мы не знаем есть ли какие-то классы или их нет.***

![7](C:\Users\user\Desktop\Лекция 5\7.png)

Какие две точки кажутся особенными? (зелененьким обвели) Их особенность в том, что они наиболее удаленные друг от друга. Если посчитаем все возможные между всеми точками рассояния и найдем максимум, то он будет в этих двух точках.

Снова считаем расстояния. До С1 - маленькие, а до С2 - большие.

Нам наплевать в сколькимерном пространстве считать расстояния между точками

![8](C:\Users\user\Desktop\Лекция 5\8.png)

Если n=2 - теорема Пифагора

Если n=3, берем ту же формулу, x и у - лежащий на столе и на полу шарик. Начало координат в углу комнаты. Один конец рулетки от шарика на столе, другой конец - шарик на полу и получим то же расстояние, что и по формуле.

Если n=4, то с рулеткой не залезем. И после 4-х нам уже наплевать.

Расстояние мы это посчитаем:

```
      ⍴ab
16 100
      dist←{(+/(⍺-⍵)*2)*.5}
```

есть точки 

```
      1 2
1 2
      3 4
3 4
```

какое между ними расстояние? будет корень из 8(рисунок)

```
      8*.5
2.828427125
```

*Имея dist ф-ции считаем расстояние между всеми a и b*

Есть два вектора векторов, внешнее произведение - выполняет попарно(слева берется первый элемент и складывается со всеми эл-тами правого аргуметра и т.д.)

```
      3 2 4 ∘.+ 5 3 
8 6
7 5
9 7
```

Прелесть в том, что мы можем любой операнд давать (умножение, сложение, вычитание), если дадим два вектора векторов в нашу ф-цию dist-вычислятся растояния между итым и житым и получаем результат в виде матрицы.

```
      3 2 4 ∘.× 5 3 
15  9
10  6
20 12
      3 2 4 ∘.- 5 3 
¯2  0
¯3 ¯1
¯1  1
      (1 2)(3 4) ∘.dist (2 1)(4 2)
1.414213562 3          
3.16227766  2.236067977
```

Превратим a и b в 16 векторов (каждый размерности 100)

```
      ⍴ab
16 100
      ab←⊂[2]ab
      ⍴ab
16
      ⍴¨ab
 100  100  100  100  100  100  100  100  100  100  100  100  100  100  100  100
```

Получили матрицу 16 на 16 парных расстояний, по диагонали матрицы 0

```
      r←ab ∘.dist ab
```

Сделаем вот такую m

```
      m←?3 3⍴9
```

Надо найти максимальный элемент в этой m (будет равно 9)

```
      m=⌈/,m         ⍝ логическая матрица,где 1=max
0 0 1
0 0 0
0 0 0
      m
4 6 9
2 6 6
7 5 7
```

Координаты точки:

```
      ⍸m=⌈/,m
1 3 
```

В нашем случае r:

```
      ⍸r=⌈/,r
3 15  15 3 
```

Т.к. матрицы симметричные возьмем из них первые i=3, j=15

```
      i j←↑⍸r=⌈/,r
      i
3
      j
15
```

r1 - расстояния всех ab до первой точки:

```
      r1←(+/(ab-ab[3])*2)*0.5
```

Так же считаем r2:

```
      r2←(+/(ab-ab[15])*2)*0.5
```

Строим:

```
      plot ∊¨r1 r2
```

![9](C:\Users\user\Desktop\Лекция 5\9.BMP)

Вот все расстояния от мервой до второй точки, т.к. опять мы понизили все наши расстояния.

Соберем все это хозяйство в ф-цию maxproj (расстояние через бве максимальные равноудаленные точки) и проверим:

```
      ⎕vr'maxproj'
     ∇ r←maxproj ab;i;j;r1;r2  
[1]    r←ab∘.{+/(⍺-⍵)*2}ab     
[2]    i j←↑⍸r=⌈/,r            
[3]    r1←(+/¨(ab-ab[i])*2)*0.5
[4]    r2←(+/¨(ab-ab[j])*2)*0.5
[5]    r←r1 r2                 
     ∇                         
      r1 r2←maxproj ab
      ⍴¨r1 r2
 16  16 
      plot r1 r2
      plot maxproj ab
```

![9](C:\Users\user\Desktop\Лекция 5\9.BMP)

# Метод Орлочи

Еще один метод понижения размерности пространства.

Мы рассмотрели:

1. Расстояние до центров двух классов;

2. Если выборка не классифицирована, не знаем, есть там классы или нет, то мы определяем максимально удаленные друг от дружки точеки и расстояние от них до точек исходной выборки нам дает картинку на плоскости.

Орлочи, это женщина, которая имела такую фамилию и предложила такой хороший метод.

Рассмотрим пример на плоскости. У нас есть 5 точек на плоскости, мы находим две наиболее удаленные друг от друга точки, их координаты (x_i,x_j), мы их всех точек вычитаем первую, т.е. x_i и первой координатой будет x_j-x_i это e_1-первая ось Орлочи, уже имея эту координату и проектируя эти точки, можем увидеть одномерное распределение вдоль этой оси, построить для него гитограмму и посмотреть, есть ли какие-то особенности или группировки.

Мы не будем ограничиваться одномерным случаем и проведем вторую ось Орлочи, чтоб получить плоскость, вторая ось проводится через точку максимально удаленную от первой оси, через точку x_k и перпендикулярно первой оси е_2

![1.1](C:\Users\user\Desktop\Лекция 6\1.1.png)

Если мы рассмотрим проекции e_1 и e_2, то получится. Отметим точки k, i и j, одна точка лежит практически на оси е_1, а другая будет с отрицательной координатой, такое мы получим распределение, когда мы говорим о двумерном примере, то, кроме того, как повернуть это распределение на плоскости, мы ничего не сделаем.

![1.2](C:\Users\user\Desktop\Лекция 6\1.2.png)

#### Переходим к многомерному случаю.

Целью методов многомерного шкалирования является представление исходных данных в пространстве небольшой размерности с сохранением, по возможности, структуры выборки (взаимного расположения точек). Рассматриваемый в насто- ящем параграфе метод - простая ординация Орлочи является элементарным по своей сути. 

Наибольшее изменение выборки должно быть вдоль оси, соединяющей две наиболее удаленные точки. 

**Описание алгоритма**

Пусть в n-мерном пространстве признаков заданы m образов:

![1](C:\Users\user\Desktop\Лекция 6\1.png)

Вычислим евклидовы расстояния между всеми точками (образами):

![2](C:\Users\user\Desktop\Лекция 6\2.png)

где x_ik - k-ая координата образа х_i. 

Определим пару наиболее удаленных точек x_i и x_j:

![3](C:\Users\user\Desktop\Лекция 6\3.png)

Иллюстрация к выводу проекций Орлочи 

Проведём первую ось e_1 искомого подпространства через эти наиболее удаленные точки, перенеся начало координат в точку х_i. Это соответствует переходу к y_k= x_k - x_i, причём ось e_1 совпадает с вектором y_j.

##### Понижение размерности данных и визуализация

Рассчитаем теперь расстояние всех точек выборки до оси e_j. Пусть перпендикуляр у, опущенный из конца вектора x_k на ось e_1 опирается на конец вектора ae_1, где а — неизвестный множитель. Из условия ортогональности ![4](C:\Users\user\Desktop\Лекция 6\4.png) откуда определяется ![5](C:\Users\user\Desktop\Лекция 6\5.png)

Найдя а, можно определить расстояние от точки x_k до оси e_1:

![6](C:\Users\user\Desktop\Лекция 6\6.png)

Определим теперь наиболее удаленную от оси e_1, точку и проведем вторую ось e_2 искомого полпространства через эту точку перпендикулярно к оси e_1. Если таковой оказалась точка x_k, то необходимо перенести начало координат в точку ae_1, что приведёт к осям:  ![7](C:\Users\user\Desktop\Лекция 6\7.png)и ![8](C:\Users\user\Desktop\Лекция 6\8.png)

##### Численный пример 

Программная реализация:

![9](C:\Users\user\Desktop\Лекция 6\9.png)

#### Линейное шкалирование Орлочи

Проекция на первую ось. Пример 200-мерный.

![10](C:\Users\user\Desktop\Лекция 6\10.png)

1-превратили матрицу спектров аккустических шумов 40х200 в вектор векторов

2-вычислили евклидово расстояние между всеми точками

3- нашли координаты наиболее удаленных точке (х_i=20,x_j=103) 

4- посчитали у отняли от всех х 20-ую точку

5-второй координатой будет 103, считаем расстояние проекции на нее

6-строим гистограмму этой проекции

![11](C:\Users\user\Desktop\Лекция 6\11.png)

![12](C:\Users\user\Desktop\Лекция 6\12.png)

На распределении видим, что все группируется в два кластера, которые надо итерпретировать.

##### Далее смотрим искусственный примерчик

```
      x←(?3 2⍴5),[1]6+4 2⍴5
      0 plot ⊂[1]x
      x←(?3 2⍴5),[1]6+?4 2⍴5
      0 plot ⊂[1]x
      x←(?3 2⍴5),[1]6+?4 2⍴5
      0 plot ⊂[1]x
      x←(?3 2⍴5),[1]6+?4 2⍴5
      0 plot ⊂[1]x
      x0←x
      x0
4   4
1   4
5   2
8  10
10 11
8   8
11 10  
      x←⊂[2]x
      disp x
.→-----------------------------------------------.
| .→--. .→--. .→--. .→---. .→----. .→--. .→----. |
| |4 4| |1 4| |5 2| |8 10| |10 11| |8 8| |11 10| |
| '~--' '~--' '~--' '~---' '~----' '~--' '~----' |
'∊-----------------------------------------------'
      byaka←disp x∘.-x
      byaka←disp (x∘.-x)*2
      byaka←disp +/ ̈(x∘.-x)*2
      x4←4⊃x
      x2←2⊃x
      x4 x2
      8 10 1 4
      8 10 - 1 4
7 6
      (8 10 - 1 4)*2
49 36
      +/(8 10 - 1 4)*2
85
      r← +/ ̈(x∘.-x)*2
      ijof←{1+(⍴⍵)⊤ ̄1+(,⍵)⍳⍺}
      ijof←{1+(⍴⍺)⊤ ̄1+(,⍺)⍳⍵}
      r ijof ⌈/,r
2 7
)fns
ijof
      COLOR 'RED'
      x[2 7]
1 4 11 10
      DRAW ⊃x[2 7]
      x1←x
      x←x-x[2]
      disp x1
.→-----------------------------------------------.
| .→--. .→--. .→--. .→---. .→----. .→--. .→----. |
| |4 4| |1 4| |5 2| |8 10| |10 11| |8 8| |11 10| |
| '~--' '~--' '~--' '~---' '~----' '~--' '~----' |
'∊-----------------------------------------------'
      disp x
.→--------------------------------------------.
| .→--. .→--. .→---. .→--. .→--. .→--. .→---. |
| |3 0| |0 0| |4  ̄2| |7 6| |9 7| |7 4| |10 6| |
| '~--' '~--' '~---' '~--' '~--' '~--' '~---' |
'∊--------------------------------------------'
      0 plot ⊂[1]⊃x
      COLOR 'RED'
      DRAW ⊃x[2 7]
      x×x[7]
30 0 0 0 40  ̄12 70 36 90 42 70 24 100 36
      +/ ̈x×x[7]
30 0 28 106 132 94 136
      proj←+/ ̈x×x[7]
      oi'mds'
      orl←mds.Orloci x0
      0 plot ⊂[2]orl
```

![13](C:\Users\user\Desktop\Лекция 6\13.png)

```
      o←orl ⊂[2]x0
      ⍴o
2
      0 plot o
```

#### Трассировка Орлочи

```
      0 plot o
```

![14](C:\Users\user\Desktop\Лекция 6\14.png)

```
      orl ⊂[2]x0
      i j
      COLOR 'RED'
      MARKER ⊃x[i j]
      DRAW ⊃x[i j]
      k
3
      MARKER ⊃x[k]
      COLOR 'BLUE'
      MARKER ⊃x[k]
      a[k]
0.2058823529
      x[j]×a[k]
2.264705882 2.058823529
      x[i]+x[j]×a[k]
3.264705882 6.058823529
      x[i]+y[j]×a[k]
3.058823529 5.235294118
      DRAW ⊃x[k],⊂3.058823529 5.235294118
```

![15](C:\Users\user\Desktop\Лекция 6\15.png)

***Пример для спектров варианте 13***

```
      ⍴s
40 200
      oi'mds'
      o←mds.Orloci s
      ⍴c
40
      c plotc ⊂[2]o
```

![16](C:\Users\user\Desktop\Лекция 6\16.png)

### Мой вариант для Орлочи - V2

Ручное исполнение:

![17](C:\Users\user\Desktop\Лекция 6\17.png)

Исполнение на APL:

```
      x←(4 5 8)(4 2 9)(2 6 6)(1 1 7)
      R←+/ ̈(x∘.-x)*2
      ⍸R=⌈/,R
2 3 3 2
      R
0 10 9 26
10 0 29 14
9 29 0 27
26 14 27 0
      x
4 5 8 4 2 9 2 6 6 1 1 7
      x-x[2]
0 3  ̄1 0 0 0  ̄2 4  ̄3  ̄3  ̄1  ̄2
      y← x-x[2]
      y×y[3]
0 12 3 0 0 0 4 16 9 6  ̄4 6
      +/ ̈ y×y[3]
15 0 29 8
```

Посчитали правильно.

# Метод главных компонент (PCA)

Математически обоснованный метод. Это классика, он присутствует как программа во всех без исключения пакетах и библиотеках для всех языков в том числе для Python, R.

Есть система координат в n-мерном пространстве

<img src="C:\Users\user\Desktop\Лекция 7\1.PNG" style="zoom:80%;" />

и эти координатные оси обладают свойствами:

1. ​	перпендикулярны друг к другу;
2. ​	<img src="C:\Users\user\Desktop\Лекция 7\3.PNG" style="zoom:80%;" /> ;
3. ​	каждая из осей имеет длину = 1  <img src="C:\Users\user\Desktop\Лекция 7\2.PNG" style="zoom:80%;" /> .

В этой системе координат у нас задано N точек, каждая точка задана вектором.

<img src="C:\Users\user\Desktop\Лекция 7\4.PNG" style="zoom:80%;" />

Мы эти векторы:

1. отцентрировали, т.е. вычли из них среднее по всем векторам, т.о. среднее стало равно 0, стрелка над нулем обозначает, что это n штук нулей, это вектор состоящий из нулевых координат.

<img src="C:\Users\user\Desktop\Лекция 7\5.PNG" style="zoom:80%;" />

2. отнормировали на длину, каждое  <img src="C:\Users\user\Desktop\Лекция 7\x_i.PNG" style="zoom:70%;" /> 	разделили на сумму квадратов и т.о. длина каждого векторочка стала равна 1

<img src="C:\Users\user\Desktop\Лекция 7\6.PNG" style="zoom:80%;" />

Вводится понятие веса каждой оси и под весом понимается дисперсия проекции на эту ось всех наших точек, т.е. суммирование идет по j от единицы до N, <img src="C:\Users\user\Desktop\Лекция 7\e_i.PNG" style="zoom: 70%;" /> – одна и та же ось для которой мы вычисляем вес, а  <img src="C:\Users\user\Desktop\Лекция 7\x_j.PNG" style="zoom:70%;" /> бегает по всем векторочкам.

<img src="C:\Users\user\Desktop\Лекция 7\7.PNG" style="zoom:80%;" />

Среднее для проекции будет равно 0, поэтому мы его не вычитаем из  <img src="C:\Users\user\Desktop\Лекция 7\7.1.png" style="zoom:67%;" /> , остается сумма квадратов проекций.

Нас волнует на сколько эти веса неравномерно распределены или наоборот – равномерно по осям координат и мерой равномерности/неравномерности является энтропия, т.е. суммы весов умноженные на логарифмы весов в принципе, т.к. мы будем для разных систем координат считать энтропию, нам напревать по какому основанию логарифм берется, но т.к. энтропия считалась ранее для бинарных данных и связывалась информацией передаваемой по каналу и там был двоичный логарифм, то и мы для определенности будем использовать двоичный.

<img src="C:\Users\user\Desktop\Лекция 7\8.PNG" style="zoom:80%;" />

Чем меньше энтропия, тем более неравномерно распределены веса координат.

Переходим к решению.

Доказывается математически, что минимуму энтропии соответствует система координат составленная из собственных векторов ковариационной матрицы.

<img src="C:\Users\user\Desktop\Лекция 7\9.PNG" style="zoom:25%;" /> , где **С** – ковариационная матрица.

Если мы умножаем какой-то вектор на матрицу, то мы получаем другой вектор в общем случае, который не совпадает с первым, не только по длине, а самое главное по направлению.

Если речь идет о собственном векторе, то его умножение на матрицу эквивалентно умножению его на какой-то скаляр, т.е. у этого вектора останется то же направление, как у исходного, изменится длина в зависимости от того, кто такой **λ** и **λ** называется собственным числом, а  <img src="C:\Users\user\Desktop\Лекция 7\e.PNG" style="zoom: 18%;" /> – собственный вектор.

Если мы вычислим у квадратной матрицы **n**x**n** имеется n штук разных собственных векторов, каждому из которых соответствует свое собственное число. 

Когда все **n** собственных векторов определим  <img src="C:\Users\user\Desktop\Лекция 7\1.PNG" style="zoom:70%;" /> . Расположим их в порядке убывания собственных чисел  <img src="C:\Users\user\Desktop\Лекция 7\10.PNG" style="zoom:60%;" /> , а перед тем, как располагать, обратим внимание на то, что собственное число равно дисперсии проекций на соответствующую ось  <img src="C:\Users\user\Desktop\Лекция 7\7.2.PNG" style="zoom:60%;" /> , отбросим k штук с наименьшими весами, т.е. имеющие наименьшие собственные числа, то ошибка представления исходного множества точек может быть записано в процентах <img src="C:\Users\user\Desktop\Лекция 7\12.PNG" style="zoom:60%;" />(сумму от 1 до (n-k) – сколько у нас осталось собственных чисел, делим на сумму всех собственных чисел, умножим на 100, вот это вот в процентах наша ошибочка представления исходных данных.)

**Рассмотрим примеры:**

Есть ковариационная матрица, чтобы не мучиться с коэффициентами ковариации, мы рассмотрим корреляционную матрицу <img src="C:\Users\user\Desktop\Лекция 7\13.PNG" style="zoom:60%;" />, т.е. у нас двумерное пространство  <img src="C:\Users\user\Desktop\Лекция 7\e_1.PNG" style="zoom:70%;" /> и  <img src="C:\Users\user\Desktop\Лекция 7\e_2.PNG" style="zoom:70%;" /> , есть какой-то набор **Х** и коэф. корреляции между  <img src="C:\Users\user\Desktop\Лекция 7\x_1.PNG" style="zoom:67%;" /> и  <img src="C:\Users\user\Desktop\Лекция 7\x_2.PNG" style="zoom:67%;" /> = 1, так получилось.

Записываем ур-е для собственного числа и вектора

<img src="C:\Users\user\Desktop\Лекция 7\9.PNG" style="zoom:18%;" />

Переносим содержащее неизвестное в левую часть выносим  <img src="C:\Users\user\Desktop\Лекция 7\e.PNG" style="zoom:18%;" /> , которое присутствует в обоих слагаемых за скобочки. Т.к. у нас остался скаляр **λ**, мы не можем его просто отнять от матрицы С, в линейной алгебре, если надо отнять скаляр, то он отнимается от диагональных элементов матрицы. Поэтому **λ** умножим на **I**, где **I** – единичная матрица(по диагонали единицы, вне диагонали нули)  <img src="C:\Users\user\Desktop\Лекция 7\14.PNG" style="zoom: 65%;" /> .

Если мы это дела раскроем, то получим систему линейных ур-ний, относительно неизвестных нам <img src="C:\Users\user\Desktop\Лекция 7\e_1.PNG" style="zoom:70%;" /> и  <img src="C:\Users\user\Desktop\Лекция 7\e_2.PNG" style="zoom:70%;" /> и неизвестного **λ**, эта система однородная (правые части равны 0). Чтобы однородная система имела решение, ее определитель должен = 0, т.е.  <img src="C:\Users\user\Desktop\Лекция 7\15.PNG" style="zoom:65%;" /> .

Расписываем:

<img src="C:\Users\user\Desktop\Лекция 7\16.PNG" style="zoom:60%;" />

**(С-λI)** в нашем случае =  <img src="C:\Users\user\Desktop\Лекция 7\16.1.PNG" style="zoom:50%;" /> , теперь считаем определитель этой матрицы. Произведение по основной диагонали это <img src="C:\Users\user\Desktop\Лекция 7\16.2.png" style="zoom:60%;" /> минус произведение по дополнительной диагонали (это 1), и это все равно 0.

Решаем получившееся квадратное ур-е, видим два решения:

<img src="C:\Users\user\Desktop\Лекция 7\17.PNG" style="zoom:50%;" />

Сколько у нас разных неизвестных? Две штуки, потому что размерность пространства у нас 2, размер матрицы ковариационной 2х2. Отсюда два собственных числа получились.

Подставляем первое **λ** в систему ур-ний <img src="C:\Users\user\Desktop\Лекция 7\18.PNG" style="zoom:65%;" /> и решаем ее

<img src="C:\Users\user\Desktop\Лекция 7\19.PNG" style="zoom:60%;" />

Этой системе из двух ур-ний с двумя неизвестными удовлетворяет любое  <img src="C:\Users\user\Desktop\Лекция 7\e_1.PNG" style="zoom:70%;" /> =  <img src="C:\Users\user\Desktop\Лекция 7\e_2.PNG" style="zoom:70%;" /> =const.

Теперь обозначим **const** как **с**  (<img src="C:\Users\user\Desktop\Лекция 7\20.PNG" style="zoom:70%;" />) и вот первый собственный вектор  <img src="C:\Users\user\Desktop\Лекция 7\21.PNG" style="zoom:65%;" />.

<img src="C:\Users\user\Desktop\Лекция 7\22.png" style="zoom:80%;" />

Как определять как брать **С**? Глубоко плевать, но для определенности возьмем его таким, чтоб длина   <img src="C:\Users\user\Desktop\Лекция 7\e_1.PNG" style="zoom:70%;" /> =1.

Разделим **С** на длину   <img src="C:\Users\user\Desktop\Лекция 7\21.PNG" style="zoom:60%;" /> =с^2+c^2=2c^2, извлекаем корень с2^.5, поделим на длину все компоненты, получим  <img src="C:\Users\user\Desktop\Лекция 7\23.PNG" style="zoom:60%;" />

Проверка APL:

```
      (2 2⍴1)+.×,['']÷2 2*.5
1.414213562
1.414213562
      ÷2 2*.5
0.7071067812 0.7071067812
      2×÷2 2*.5
1.414213562 1.414213562
```

Теперь подставляем второе λ = 0, ищем второй собственный вектор аналогично первому.

<img src="C:\Users\user\Desktop\Лекция 7\24.PNG" style="zoom:70%;" />

Получаем два линейных ур-ния:

<img src="C:\Users\user\Desktop\Лекция 7\25.PNG" style="zoom:60%;" />

<img src="C:\Users\user\Desktop\Лекция 7\26.PNG" style="zoom:60%;" />

Решение - та же константа, то с противоположными знаками

<img src="C:\Users\user\Desktop\Лекция 7\27.PNG" style="zoom:60%;" />

<img src="C:\Users\user\Desktop\Лекция 7\28.PNG" style="zoom:60%;" />

<img src="C:\Users\user\Desktop\Лекция 7\29.PNG" style="zoom:60%;" />

Проверка на APL:

```
      (2 2⍴1)+.×,['']1 ￣1×÷2 2*.5
0
```

Что такое единичная корреляционная матрица, и что такое собственные числа 2 и 0, и что такое собственные векторы  <img src="C:\Users\user\Desktop\Лекция 7\29.1.PNG" style="zoom:60%;" /> и  <img src="C:\Users\user\Desktop\Лекция 7\29.2.PNG" style="zoom:60%;" /> ?

<img src="C:\Users\user\Desktop\Лекция 7\30.png" style="zoom:70%;" />

Как будут лежать точки  <img src="C:\Users\user\Desktop\Лекция 7\x_1.PNG" style="zoom:60%;" /> , <img src="C:\Users\user\Desktop\Лекция 7\x_2.PNG" style="zoom:60%;" />  для **ρ**<img src="C:\Users\user\Desktop\Лекция 7\x_1.PNG" style="zoom:60%;" /><img src="C:\Users\user\Desktop\Лекция 7\x_2.PNG" style="zoom:60%;" />=1?

<img src="C:\Users\user\Desktop\Лекция 7\31.png" style="zoom:80%;" />

Они лежат на прямой под углом 45 градусов.

Т.е. <img src="C:\Users\user\Desktop\Лекция 7\x_2.PNG" style="zoom:60%;" /> =0+1(коэф.наклона) х  <img src="C:\Users\user\Desktop\Лекция 7\x_1.PNG" style="zoom:60%;" />=<img src="C:\Users\user\Desktop\Лекция 7\x_1.PNG" style="zoom:60%;" />

Далее, среднее у них после центрирования = 0, соответственно коэф. корреляции равен <img src="C:\Users\user\Desktop\Лекция 7\32.png" style="zoom:90%;" /> и это будет  <img src="C:\Users\user\Desktop\Лекция 7\33.png" style="zoom:80%;" />

Если мы посчитаем проекции на  <img src="C:\Users\user\Desktop\Лекция 7\e_1.PNG" style="zoom:60%;" /> , опустим их так, что если сначала они были под 45 градусов, теперь будут под 0 градусов.  <img src="C:\Users\user\Desktop\Лекция 7\e_2.PNG" style="zoom:60%;" /> перпендикулярный к  <img src="C:\Users\user\Desktop\Лекция 7\e_1.PNG" style="zoom:60%;" /> . Все проекции на  <img src="C:\Users\user\Desktop\Лекция 7\e_2.PNG" style="zoom:60%;" /> будут равны 0.

<img src="C:\Users\user\Desktop\Лекция 7\34.png" style="zoom:90%;" />

Значит мы можем, если **k** возьмем = 1, т.е. отбросим λ=0, то сумма от единицы до **(n-k)** будет равно 2, сумма от 1 до n =2+0=2, соответственно 2/2x100%=100%, то бишь оставляя первый собственный вектор соответствующий собственному числу мы сохраняем 100% информации о взаимном распределении точек, ничего не теряем. Действительно, накий пес нам играть в две координаты  <img src="C:\Users\user\Desktop\Лекция 7\x_1.PNG" style="zoom:60%;" /> и  <img src="C:\Users\user\Desktop\Лекция 7\x_2.PNG" style="zoom:60%;" /> , если проекция на  <img src="C:\Users\user\Desktop\Лекция 7\e_1.PNG" style="zoom:60%;" />- одномерная и все расстояния между точками сохраняются, мы видим всю структуру.

**Пример 2:**

Корреляционная матрица диагональная. Т.е. коэф. корреляции между  <img src="C:\Users\user\Desktop\Лекция 7\x_1.PNG" style="zoom:60%;" /> и  <img src="C:\Users\user\Desktop\Лекция 7\x_2.PNG" style="zoom:60%;" /> = 0. Соответственно, вычисление аналогично:

<img src="C:\Users\user\Desktop\Лекция 7\35.PNG" style="zoom:60%;" />

Решением будет:

<img src="C:\Users\user\Desktop\Лекция 7\36.PNG" style="zoom:60%;" />

Находим собственные векторы, подставляя первое λ, матрица становится нулевой:

<img src="C:\Users\user\Desktop\Лекция 7\37.PNG" style="zoom:60%;" />

Соответственно любые  <img src="C:\Users\user\Desktop\Лекция 7\e_1.PNG" style="zoom:60%;" /> и  <img src="C:\Users\user\Desktop\Лекция 7\e_2.PNG" style="zoom:60%;" /> будут удовлетворять: 

<img src="C:\Users\user\Desktop\Лекция 7\38.PNG" style="zoom:60%;" />

Единственное, что нас волнует, это то, что эти вектора <img src="C:\Users\user\Desktop\Лекция 7\e_1.PNG" style="zoom:60%;" /> и  <img src="C:\Users\user\Desktop\Лекция 7\e_2.PNG" style="zoom:60%;" /> должны быть перпендикулярны между собой:

<img src="C:\Users\user\Desktop\Лекция 7\40.png" style="zoom:100%;" />

**ρ=0,** т.е. точки распределены, грубо говоря, внутри какого-то круга и когда мы считаем 1,2,3,4-квадранты, получаем сумму (X1i x X2i), т.к. у нас  <img src="C:\Users\user\Desktop\Лекция 7\39.png" style="zoom:70%;" /> , то в первом квадранте у нас будет ''+'' на ''+'', получим большое положительное, в третьем квадранте будет ''–'' на ''–'' тоже большое положительно число, во втором и четвертом квадрантах ''+'' на ''–'' получим большое отрицательное число.

Итого: положительное(1квадрант)+отрицательное(2квадрант)+ положительное (3квадрант) + отрицательное(4квадрант) = 0

И относительно этого нуля, как не крути систему координат, ничего не изменится!

И если снова обратимся к нашему ур-нию в выкинем теперь второе λ = 1, то 1/(1+1) х 100% =50%

Выкидывая любую из координат  <img src="C:\Users\user\Desktop\Лекция 7\x_1.PNG" style="zoom:60%;" /> и  <img src="C:\Users\user\Desktop\Лекция 7\x_2.PNG" style="zoom:60%;" /> , мы теряем 50% информации о распределении наших точек. Например проектируем на красную ось, во все, кто выше или ниже, а вот самые удаленные друг от друга точки сольются в одну точку.

**Рассмотрим искусственный примерчик о температурном поле:**

Есть реактор, где у нас 1, 2,...., 10 термопары на выходе и какое-то поле, слева холоднее, справа горячее. 

<img src="C:\Users\user\Desktop\Лекция 7\41.png" style="zoom:100%;" />

Если мы в разные моменты времени проведем измерения, то это поле сохраняя распределение, что слева холоднее, справа горячее, может еще быть все больше, все меньше, т.е. оно не меняя распределения по термопарам, а средняя температура может быть то больше, то меньше.

<img src="C:\Users\user\Desktop\Лекция 7\42.png" style="zoom:100%;" />

Функция которая на APL возвращает три результата:

а1 – собственные числа

b1 – матрица собственных векторов 

c1 – матрица проекций на собственные вектора исходных данных, в данном случае х1.

```
      x1←⍳10
      x1←⊃(⊂x1)+⍳16
      (a1 b1 c1)←mds.(COVM SELFIC)x1
```

Во-первых, нам очень интересно, какие получились первые 10 чисел

```
      a1
212.5 0 0 0 0 0 0 0 0 0
```

Т.к. матрица ковариационная, то собственные числа нецелочисленные и получилось, что одно собственное число 212.5 отличается от 0, все остальные равны 0.

Если мы отбросим 9 собственных векторов и оставим один, который соответствует первому собственному числу, которое не нулевое. Мы сохраним 100% информации(212.5/(212.5+0+0+0+0+0+0+0+0+0)х100%).

Во-вторых, нам интересно посмотреть на первый собственный вектор, что это за голубь на который мы должны проектировать?

```
      b1[;1]
¯0.316227766 ¯0.316227766 ¯0.316227766 ¯0.316227766 ¯0.316227766
¯0.316227766 ¯0.316227766 ¯0.316227766 ¯0.316227766 ¯0.316227766
```

Первое, что мы должны, что все компоненты этого собственного вектора в 10-мерном пространстве равны между собой по модулю и просто равны. Второе, они все отрицательные, ну и плевать, то бишь, проекции на этот собственный вектор будет (-0.316227766)хТ1+(-0.316227766)хТ2+…+(-0.316227766)хТ10, т.е., если рассматривать эти 0.316227766 несмотря на минус, как веса, то получаем, что в качестве проекции  у нас взвешенное с этими весами, одинаковыми сумма всех наших термопар, которые определяют на сколько у нас отличается среднее значение температуры. Т.е. на самом деле у нас не 10-мерное пространство, а одномерное, где средний ур-нь меняется.

<img src="C:\Users\user\Desktop\Лекция 7\43.png" style="zoom:100%;" />

Теперь рассмотрим видоизмененный этот пример, где у нас меняется не только средний уровень, но и распределение, т.е.:

<img src="C:\Users\user\Desktop\Лекция 7\42.1.png" style="zoom:100%;" />

В каком-то эксперименте мы одно наблюли, в другом — другое и т.д.

Т.е. у нас меняются две вещи:

1. средняя температура;

2. распределение температур (слева — холодно, справа — горячо или наоборот).

Для этого случая посчитаем собственные числа и векторы.

Во-первых, мы видим, что у нас два собственных числа не равны 0 (первое число и второе число), остальные = 0. Мы можем безболезненно, не теряя информации, понизить нашу размерность пространства с 10-мерной до 2-мерной.

```
      x2←⍳10
      x2←⊃(⊂x2)×∊2⍴⊂⍳8
      x2[⍳8;]←⌽x2[⍳8;]
      (a2 b2 c2)←mds.(COVM SELFIC)x2
      a2
2103.75 1588.125 2.479131558E¯13 8.187234765E¯14 6.036530033E¯14 1.97029597E¯14 1.239658888E¯14 ¯3.340844541E¯14 ¯5.933993521E¯14 ¯1.555394639E¯13
```

Во-вторых, очень интересно на какие собственные векторы мы будем проектировать:

```
      b2[;⍳2]
0.4954336943        ¯0.316227766
0.3853373178        ¯0.316227766
0.2752409413        ¯0.316227766
0.1651445648        ¯0.316227766
0.05504818826       ¯0.316227766
¯0.05504818826      ¯0.316227766
¯0.1651445648       ¯0.316227766
¯0.2752409413       ¯0.316227766
¯0.3853373178       ¯0.316227766
¯0.4954336943       ¯0.316227766
```

Начнем со второго. Второй нам даст, при проектировании 10-ти термопарок, опять видим одинаковые множители, т.е. он даст аналог средней температуры на выходе.

А вот первый поинтереснее, мы видим, что первая компонента, соответствующая первой термопаре имеет какое-то положительное значение, а последняя компонента (10-ая термопара) имеет точно такое же по модулю значение, но отрицательное, т.е. когда мы начнем проектировать, мы из первой термопары умноженной на 0.4954336943 вычтем 10-ую умноженную на 0.4954336943, т.е. получим первое минус десятое, эта разность будет характеризовать перекос поля:

<img src="C:\Users\user\Desktop\Лекция 7\44.png" style="zoom:100%;" />

Теперь идя к центру реактора, сохраняется одинаковость весов и противоположность знаков, но по модулю эти веса убывают, это связано с тем, что в середине реактора не зависимо от наклона у нас маленькие изменения, а по краям — большие изменения в зависимости от направления поля. 5 и 6 термопары будут иметь минимальный вес, а крайние — максимальный, это перекос. Первое собственное число = перекос, второе собственное число = среднее значение поля.

<img src="C:\Users\user\Desktop\Лекция 7\45.png" style="zoom:100%;" />

# Sammon

Мы имеем в *n*-*мерном* пространстве набор точек векторов ![](C:\Users\user\Desktop\Лекция 8\1.png), где N – штук. Имеем на плоскости набор точек вектора заданные ![](C:\Users\user\Desktop\Лекция 8\2.png).

Для каждой пары точек ![](C:\Users\user\Desktop\Лекция 8\3.png) и ![](C:\Users\user\Desktop\Лекция 8\4.png) в исходном *n*-*мерном* пространстве мы можем посчитать расстояние между ними, по формуле

![](C:\Users\user\Desktop\Лекция 8\5.png)

 ![](C:\Users\user\Desktop\Лекция 8\7.jpg)

Тоже самое можем делать в двумерном пространстве (расстояние между у). 

*–плоскость

![](C:\Users\user\Desktop\Лекция 8\6.png)

 ![](C:\Users\user\Desktop\Лекция 8\8.jpg)

В чем простая по смыслу, но трудоемкая в вычислительном плане, идея Сэммона?

В том, чтобы расположить точки на плоскости так, чтобы взаимные расстояния * между  ![](C:\Users\user\Desktop\Лекция 8\3.png)и  ![](C:\Users\user\Desktop\Лекция 8\4.png)точками максимально соответствовали расстояниям в исходном *n*-*мерном* пространстве. Т.е. те точки, которые в исходном *n*-*мерном* пространстве далеко друг от друга, должны быть и на плоскости далеко друг от друга и наоборот.

Критерием этого, чтобы кто далеко в *n*-*мерном* был и на плоскости далеко или наоборот, является следующее.

![](C:\Users\user\Desktop\Лекция 8\11.png)

Обратим внимание на разность расстояния в *n*-*мерном* и расстояния на плоскости взятую в квадрате.

Т.к. матрица расстояний симметричная, т.е.

![](C:\Users\user\Desktop\Лекция 8\9.jpg)

Для определенности берем нижний треугольник и это обусловлено суммированием, где *i<j*, т.о. задается нижний треугольник.

Т.е. мы должны оптимизировать квадраты разностей между расстояниями в исходном *n*-*мерном* пространстве и на плоскости.

Что делает /![](C:\Users\user\Desktop\Лекция 8\12.png)? Он делает разности относительными, если в исходном n-мерном пространстве ![](C:\Users\user\Desktop\Лекция 8\12.png) большое, то разность может быть побольше, они и так далеко друг от дружки и эта разность может быть побольше, а там, где они маленькие, то чувствительность к разности становится сильнее.

![](C:\Users\user\Desktop\Лекция 8\13.png) – это некий нормировочный множитель , когда мы все то хозяйство доделили на сумму всех ![](C:\Users\user\Desktop\Лекция 8\12.png). Вся любовь.

И для минимизации этого критерия, нужно минимизировать его, т.е. с ![](C:\Users\user\Desktop\Лекция 8\12.png) мы ничего не можем делать, а с ![](C:\Users\user\Desktop\Лекция 8\12.png)* мы можем двигать в разные стороны, т.е. если какие-то ***i j*** точки оказались близко между собой, а в исходном пространстве они далеко, мы их просто раздвинем и наоборот.

Откуда берутся ![](C:\Users\user\Desktop\Лекция 8\2.png)? Один из простых вариантов – мы на плоскость N точек высыпаем случайно. Например, на ватмане рисуем систему координат y1 y2 и высыпаем N зерен на плоскость. Далее нумеруем эти зерна в произвольном порядке каждое и после, посчитав критерий ξ, начать раздвигать зернышки, которые упали рядом, а в *n*-*мерном* пространстве они далеко друг от друга или сдвигать те, которые на плоскости упали далеко, а в n-мерном пространстве они близко.

Реализация на APL:

```
[0] z←{y}sam x;e;i;j;k;dx;dy;m;a;c;l
[1] a←0.35  ⍝ [.3,.4] Sammon told
[2] ⍎(0=⎕NC'y')/'y←⍉mds.Orloci x' 
[3] dx←dist x
[4] m←,m°.<m←⍳↑⍴dx 
[5] dy←dist y
[6] e←⍬
[7] L:e,←(÷+/m/,dx)×+/(m/,(dx-dy)*2)÷m/,dx
[8] :For i :In ⍳↑⍴y
[9] l←i≠⍳↑⍴y 
[10] c←(l⌿(y[i;]-[2]y)×[1]dx[i;]-dy[i;])÷[1]l/dx[i;]×dy[i;]
[11] y[i;]←y[i;]+(2×a÷+/m/,dx)×+⌿c 
[12] :EndFor 
[13] dy←dist y 
[14] →(1000>⍴e)/L
[15] z←e y
```

Правый аргумент от *sam* – вектор векторов n-мерных, а левый аргумент от *sam* в фигурных скобках означает опциональность, т.е. он может быть, а может не быть. Если он есть, то откуда-то мы на плоскости эти N точек поместили. А если мы не задаем левый аргумент, то в качестве левого приближения используются проекции на оси Орлочи. Орлочи нам как-то скорректирует, а дальше начинаем оптимизировать, далее идет цикл, т.к. при градиентной оптимизации без цикла никак.

Рассмотрим пример в трехмерном пространстве:

![](C:\Users\user\Desktop\Лекция 8\10.jpg)

```
      ⍴xy                        ⍝вершины этих кубиков
8 3
      xy                         ⍝матрица из координат вершин
0 0 0
1 0 0
0 1 0
0 0 1
0 1 1
1 0 1
1 1 0
1 1 1
      (⍳8) plotc ⊂[2]Orloci xy   ⍝спроектируем ее с помощью Орлочи
```

![](C:\Users\user\Desktop\Лекция 8\14.png)

Специально делаем каждую точку своим маркером и цветом, если бы мы так не сделали, то на проекции Орлочи мы бы увидели 5 точек вместо 8-ми. А так видим, что при линейном проектировании товарища Орлочи некоторые точки совпали, это фигово, в том плане, что расстояние между любой парой вершин не меньше 1, а тут вышло, что между какими-то вершинами расстояние 0.

Теперь зовем товарища Сэммона для проектирования.

```
      z←sam xy
      (⍳8) plotc ⊂[1]2⊃z
```

![](C:\Users\user\Desktop\Лекция 8\15.png)

Результат – все наши 8 точек на месте, так выглядит кубик при проектировании на плоскость.

Представим, что в каждой вершине кубика несколько точек соответствующие одному из 8-ми классов:

```
      xy1←⊂[2]xy
      xy2←xy1{⍺,[1]⍺+[2]⍵}¨⊂[2 3]8 5 3⍴0 0.1 stat.rndn×/8 5 3
      ⍴xy2←,[1 2]⊃xy2
48 3
      (∊6⍴¨⍳8) plotc ⊂[2]mds.Orloci xy2
```

![](C:\Users\user\Desktop\Лекция 8\16.png)

Вот у нас 8 болезней и больные болезнью номер 1, попали в одну вершину кубика, больные болезнью 2 в другую и т.д. И мы с помощью Орлочи это спроектировали на плоскость. Но можем различить только 4 болезни из 8-ми, потому что как и с точками у нас одна болезнь наложилась на другую и мы не можем их различить. Это очень плохо как для медицинской, так и технической диагностики.

Эти же данные спроектируем на оси Сэммона

```
      z2←(?48 2⍴100)sam xy2
      plot 200↑↓z2    ⍝ как меняется критерий ξ при оптимизации
```

![](C:\Users\user\Desktop\Лекция 8\17.png)

```
      (∊6⍴¨⍳8) plotc ⊂[1]2⊃z2
```

![](C:\Users\user\Desktop\Лекция 8\18.png)

Видим, что все 8 разных классов друг от дружки отстоят далеко и, используя метод распознавания (эталонов – среднее по всем точкам принадлежащим данному классу), можем распознать центры для каждого класса болезней. Метод эталонов заключается в том, что мы можем поставленную точку в пространстве определить к какому-то из классов, посчитав расстояние от неизвестной точки до центра эталона и там где оно меньше, значит к тому классу мы точку и определим.

Реализация Сэммона на R:

```
> xy2<-read.table("d:/xy2.txt") 
> xy2<-as.matrix(xy2) 
> s2<-sammon(xy2)
Error in sammon(xy2) : Distances must be result of dist or a square matrix 
> s2<-sammon(dist(xy2)) 
Initial stress : 0.08900 
stress after 10 iters: 0.05915, magic = 0.500 
stress after 20 iters: 0.05881, magic = 0.500 
stress after 30 iters: 0.05871, magic = 0.500 
stress after 40 iters: 0.05864, magic = 0.500 
> plot(s2$points,type="n") 
> text(s2$points,labels=as.character(1:nrow(xy2)))
>
```

![](C:\Users\user\Desktop\Лекция 8\19.png)

```
> plot(s2$points,type="p")
```

![](C:\Users\user\Desktop\Лекция 8\20.png)

```
      ⍴s                   ⍝ взяли в качестве начального приближения какой-то пятимерный массив
47 5
      z←(?47 2⍴100)sam s   ⍝ запустили его с Сэммоном, взяв в качестве приближения случайные 47 точек на плоскости
```

Запустили на ночь, прервали утром. Прервался на 9-ой строчке.

```
      z←(2⊃z)sam s
      sam[9]
      ⍴e        ⍝видим, что за ночь 894678 итераций прошли за ночь
894678
      ee←e
      ⍴y
47 2
      yy←y
      plot ee   ⍝строим график изменений в процессе ночного бдения
```

![](C:\Users\user\Desktop\Лекция 8\21.png)

Видим как выглядят наши проекции:

```
     0 plot ⊂[1]yy
```

![](C:\Users\user\Desktop\Лекция 8\22.png)

Это, конечно, безобразие. Дело в том, что APL – язык интерпретатор.

Это означает, что мы должны какую-то строчку интерпретировать, т.е. в компилируемых языках мы должны вот эту программу, которая должна "3+4" считать, компилировать и она мухой выполнится.

В APL мы должны интерпретировать и естественно, то как мы интерпретировали эту строчку и поняли, что ***«+»*** это суммирование, у нас уже включится для плюса написанная на языке низкого уровня откомпилированная процедура, которая его мухой выполнит, но на интерпретацию уйдет время (нас это не напрягает).

```
      3+4
7
      +/(⍳1000)*2   
333833500
```

Посмотрим со стороны Basic. Результат у него будет:

```
     )ed basic 
     ∇ r←basic n;i                                                                    [1]    r←0                                                                            [2]    :For i :In ⍳n                                                                  [3]        r←r+i*2  ⍝ interpret this line n times                                    [4]    :EndFor                                                                          ∇                                                                                      basic 1000
333833500
```

 Трагизм ф-ции Basic, что в [3] будет интерпретироваться не 1 раз, а 1000 раз, следовательно ф-ции Basic в разы медленнее, чем ***«******+/******»***.

R является интерпретирующим. Он быстрый, потому что. если в APL мы Сэммона пишем по строчкам и цикл, внутри цикла интерпретировались все строчки каждый раз, именно поэтому он всю ночь телепался.

Поэтому в R, когда зовем *dist**(**xy**2)*, то эта ф-ция написана не на R, а, например, на С, из-за чего в R это является вызовом. Мы никогда не увидим, что там внутри Сэммона делается или, что внутри dist делается. Это откомпилированный код, который вместе с R поступает на компьютер при установке. Они этому коду передают только аргумента, а он быстро-быстро откомпилированный крутится, поэтому нам кажется, что R быстрее, но быстрота заключается в том, что он вызывает каждый раз функцию.

В APL реализована связь с R:

```
      )copy rconnect
      r←⎕new R   ⍝создаем новый объект
      r.init     ⍝инициализируем
RConnect initialized
```

Внутри R даны функции:

```
      'x' r.p ⍳9      ⍝помещает в R какой-то объект из APL, левый аргумент – под каким именем я его буду помещать, а правый аргумент – что за объект
      +r.g 'x'        ⍝взять из R какой-то объект
1 2 3 4 5 6 7 8 9
      +r.x 'sum(x)'   ⍝в качестве аргумента берет вызов какой-то R-ой функции каким-то аргументом. Например, X – которые засланы в R просуммируем там же
45
      +/⍳9            ⍝проверили не наврал ли R
45 
```

```
⍝ generate 3 clusters data for sammon prejections
      x1←?30 50⍴0
      x2←2+?30 50⍴0
      x3←¯2+?30 50⍴0
      ⍴xx←x1⍪x2⍪x3
90 50
      r.x'library(MASS)'
      r.x'ss<-sammon(dist(⍵))'xx
      r.x'ss<-plot(ss$points)'
      pp←r.g'ss$points'
      pp
[R matrix: 90x2 : dimnames]
      p1←r.x'p1<-pp[,1]'
      p2←r.x'p2<-pp[,2]'
      ]load plt
#.plt
      plt.plot p1 p2
      r.x'ss<-plot(ss$points)'
      'x2'r.p x2
      pp←r.x 'ss$points'
      'xx' r.p xx
      r.x'ss<-sammon(dist(xx))'
      r.x 'plot(ss$points)'
      ⎕←pp←r.x 'ss$points'
[R matrix: 90x2 : dimnames]
       ⎕←r.x 'summary(ss$points)'
 [R table - 6 rows]                        
        V1                    V2           
  Min.   :-16.994313    Min.   :-3.5412    
  1st Qu.:-13.543887    1st Qu.:-1.1105    
  Median :  0.006133    Median :-0.1200    
  Mean   :  0.000000    Mean   : 0.0000    
  3rd Qu.: 13.483473    3rd Qu.: 0.9875    
  Max.   : 16.769582    Max.   : 6.5274    
       pp←r.x 'ss$points'
      ⍴pp.Value ⍝ what I forgot it was get Value from a complex object pp
90 2
      +⌿pp.Value
¯2.675637489E¯14 ¯4.218847494E¯15
      ⌈⌿pp.Value
16.76958233 6.527426142
      ⌊⌿pp.Value
¯16.99431306 ¯3.54116532
```

# **Методы, которые помогают оценить информативность разных признаков, это мы будем делать имея У.**

Х – матрица, объект/свойство

У – в номинальной шкале название класса или номер класса, если в непрерывной шкале, то это переменная, которая зависит от наших признаков(Х)

![](C:\Users\user\Desktop\20210416\0416.png)

*Будем говорить о двух классах*

т.е. ω_к указывает на то, какой класс имеет место быть (1 или 2). И наша задача научиться классифицировать, т.е. построить такую ф-цию y=f(х), которая нам позволяет предсказать на каких-то новых данных, например, в медицинской диагностике, у-здоровый больной, то для какого-то Сидорова мы опишем набор его анализов вектором Х, подставим в эту ф-цию, которую мы обучили, используя нашу обучающую выборку и получим у=1 – здоров, у=2 – болен тем-то.

Существует масса методов (линейных/нелинейных) построения этой функции. Одни методы могут быть очень простые и идеалогические с вычислительной точки зрения, но оно и работает часто фигово. Другие методы, даже, если с точки зрения понимания нет ничего особо трудного, что именно делается, а трудность для вычислений есть и в частности, она из гаденьких задач вычислительной математики и линейной алгебры, на которых основаны большинство методов распознавания образов классификаций, построение матриц функций f, это обращение матриц.

Пока у нас какие-то детские(маленький размер) матрицы. Если возьмем все имеющиеся признаки и начнем со всеми сразу строить ф-цию f, то придется обращать матрицу 100х100 или 1000х1000, не дай Бог, потому что признаков у нас может быть бесконечное множество.

Методы распознавания образов классификаций сильно зависит от размерности. Здесь стоит задача понижения размерности пространства признаков (не обязательно до n=2!!!, но до числа n облегчающего построение ф-ции f-распознавания(классификации)).

Т.е. мы не собираемся ничего визуализировать, но хотим посмотреть как наши признаки отличаются по информативности с т.з. распознавания одного от другого и выбрать какое-то небольшое (от 100 выберем, например, 10).

### **Критерий информативности (полезности) признаков для распознавания**

Есть какой-то признак Хi – i столбец нашей матрицы Х. Заглядываем в вектор У, берем сначала все значения Х, когда был первый класс и строим по ним гистограмму (ω1-класс). По тому же Хi берем значения, соответствующие классу ω2 и стоим еще одну гистограмму.

![0416(C:\Users\user\Desktop\20210416\0416(1).png)](C:\Users\user\Desktop\20210416\0416(1).png)

То же самое делаем для признака Хj

![0416(C:\Users\user\Desktop\20210416\0416(2).png)](C:\Users\user\Desktop\20210416\0416(2).png)

Какой из признаков нам нравится больше? **Хj** !!!

Как мы можем эту хорошесть описать? Можем взять модуль разности m1 и m2 или возвести эту разность в квадрат. Т.е. мы можем оценить полезность разностью условных средних, беря ее в квадрат. 

![0416(C:\Users\user\Desktop\20210416\0416(3).png)](C:\Users\user\Desktop\20210416\0416(3).png)

Или от номера признака построить график и получить:

![0416(C:\Users\user\Desktop\20210416\0416(4).png)](C:\Users\user\Desktop\20210416\0416(4).png)

Можем сказать, что самые информативные признаки обозначены красными точками. Можем взять какой-то порог (посмотрим распределение (m1 - m2)^2, т.е. будет много признаков болтаться вокруг какого-то небольшого значения этого квадрата разности, а какая-то часть будет иметь большее чем у остальных квадрата разности средних, там и будет порог).

![0416(C:\Users\user\Desktop\20210416\0416(5).png)](C:\Users\user\Desktop\20210416\0416(5).png)

k Є множеству информативных признаков, если (m1^k - m2^k)^2 > порога. Неважно, сколько останется этих признаков, в функцию f подставляем не полный вектор Х, а Х_k Є множеству информативных признаков.

Сейчас нам не важно, какой метод мы будем использовать, возьмем самый примитивный – метод эталонов, когда в качестве эталона берем центры. Набор информативных признаков подаем и смотрим к какому центру ближе.

### **Возьмем еще два других признака**

Строим гистограммы, как делали это выше

![1](C:\Users\user\Desktop\20210416\1.jpg)

По нашему критерию признак j лучше, а по ощущению лучше i. Следовательно, мы сочинили плохой критерий!

Что мы не учли? **ИЗМЕНЧИВОСТЬ ВНУТРИ КЛАССОВ**. Т.е. для j-ого разность средних большая, но внутри значения класса меняются +/- лапоть. Соответственно, эти распределения сильно пересекаются и мы будем допускать много ошибок (точку для второго класса мы будем относить к первому и наоборот, так нельзя!)

![2](C:\Users\user\Desktop\20210416\2.jpg)

*Признак Хi имеет небольшую относительно j разность средних по классам, но внутри каждого класса они мало меняются, соответственно, **можем легко разделить два класса каким-то порогом***:

![3](C:\Users\user\Desktop\20210416\3.jpg)

**Кроме мер положения, должны использоваться меры изменчивости.**

Меры изменчивости:

- ​	среднеквадратичное отклонение
- ​	сумма квадратов
- ​	т.д.

![4](C:\Users\user\Desktop\20210416\4.jpg)

Чем S1^2 отличается от дисперсии отклонений по первому классу? Дисперсия делится на число точек в первом классе.

Мы не делим на число точек, потому что мы используем эту сумму и нас волнует общая изменчивость, если у нас представителей одного класса много, а другого – мало, то при делении на число точек они у нас будут мало отличаться. Сумма наших S будет отображать общую изменчивость в классах.

**F – критерий Фишера.**

По критерию Фишера Xi будет лучше Xj, потому огромное расстояние между средними поделится на огромную сумму квадратов отклонений по ω1 и ω2.

Например, у нас есть 100 исходных признаков, вычисляем значение Фишера для каждого из этих 100 признаков и строим или распределение значений критерия Фишера, или кривую, показывающую значение от номера признака, или и то и то вместе. На основе этих картинок выбираем небольшое число меньше исходных 100, например, 10 наиболее информативных по критерию Фишера. Дальше с ними работаем, строим функцию f для распознавания классификаций.

Звучит хорошо в случае, если у нас данные «чистые» (отсутствие ошибочных значений и выбросов), если взять любую книжку по науке о данных, но с разными названиями, они для нас эквивалентны.

Если данные «грязные», т.е. присутствуют выбросы.

Пусть изначально Fj>Fi, ошибочное значение или выбросы это

![5](C:\Users\user\Desktop\20210416\5.jpg)

К чему это приведет? m1 поедет в сторону выбросов, m2 в сторону другого выброса, таким образом разность средних у нас может оказаться меньше, чем для признака Хi и это в целом хороший признак, если выкинуть эти выбросы, то Хi будет хуже Хj.

Выбросить выбросы – значит использовать для вычисления мер положений робастные (устойчивые к выбросам и ошибкам в данных) методы оценивания мер положения.

А медиане наплевать на выбросы:

![10](C:\Users\user\Desktop\20210416\10.jpg)

Здесь мы не говорим на сколько точек больше, на сколько меньше, они могут хоть в Америку убежать, но для нас их две штуки и они попадут в те 50 больше/меньше медианы. Медиане наплевать, она со своего места никуда не сдвинется.

*Поэтому в робастном критерии Фишера мы напишем med1-med2 и возьмем модуль.*

Что делать с изменчивостью, которая тоже чувствительна к выбросам? **Нужно поставить порог.**

![6](C:\Users\user\Desktop\20210416\6.jpg)

Вместо суммы квадратов отклонений от среднего, глядя на ящик с усами, в знаменатель нужно поставить межквартильный размах для ω1 и ω2.

![11](C:\Users\user\Desktop\20210416\11.jpg)

![7](C:\Users\user\Desktop\20210416\7.jpg)

Можем посчитать для всех наших 100 признаков этот  робастный критерий Фишера. Построить его в гистограмму или кривульку с порогом, отобрать и использовать информативные признаки.

Всегда ли надо использовать робастный? НЕТ. Использовать нужно оба два. В идеале, если чистая выборка, то у нас значения будут лежать на прямой(более информативные признаки), информативные признаки по Фишеру будут лежать так же как и по робастному Фишеру

![8](C:\Users\user\Desktop\20210416\8.jpg)

Если вдруг какая-то точка по Фишеру хорошо, а по робастному она мало информативная, то значит есть выборки какие-то по этому признаку отклонения и в другую сторону тоже. Т.о. мы диагностируем проблемы в нашей выборке (какого черта Х25 сильно отличаются робастный и обычный критерии Фишера, смотрим распределение и видим выбросы, это нам помогает). 

![9](C:\Users\user\Desktop\20210416\9.jpg)

Если они похоже себя ведут, то мы выбрали множество информативных и с ним счастливо стоим функцию f.

# **Многомерный критерий Фишера**

По математике он совпадает с линейным дискриминантным анализом.

Есть двумерное пространство, где распределены два класса. Признак Х1 плох для классификации распознавания этих классов, потому что проекции на этот признак из областей двух классов смешиваются, тоже самое и с признаком Х2.

Поищем направление проекции, на которое максимизирует одномерный критерий Фишера(такое направление, чтобы критерий Фишера был максимальным) и  можно будет отделить один класс от другого. 

Нужно провести направление f под углом 135*С. Рисуем проекции наших классов на это направление. Они не пересекаются.

Мы не делим на число точек, когда считаем вариацию, чтобы если в одном классе оказалось точек много больше, чем в другом, после деления на число точек у нас S1 и S2 уравняются в своих правах. А когда просуммируем квадраты отклонений от своего среднего, то этого не происходит.

Изобразим программку, которая с шагом в 1*С будет крутить эту ось f для проектирования, на каждом градусе определит проекции, определит критерий F и возьмет те градусы, которые соответствуют максимальному критерию F. Если мы будем в трехмерном пространстве. То придется выбирать точки на сфере и придется перебирать больше направлений.

К счастью, для линейного дискриминантного анализа  и для многомерного критерия Фишера существует простая математика.

![20210423](C:\Users\user\Desktop\Лекция 10\20210423.jpg)

Оптимальное направление w для проектирования векторов x в многомерном
пространстве признаков определяется как:

<img src="C:\Users\user\Desktop\Лекция 10\20210423(5).jpg" alt="20210423(5)" style="zoom:67%;" />

<img src="C:\Users\user\Desktop\Лекция 10\20210423(6).jpg" alt="20210423(6)" style="zoom:67%;" />

<img src="C:\Users\user\Desktop\Лекция 10\20210423(7).jpg" alt="20210423(7)" style="zoom: 67%;" />

i = 1, 2 −ковариационные матрицы (с точностью до множителя <img src="C:\Users\user\Desktop\Лекция 10\20210423(11).jpg" alt="20210423(11)" style="zoom: 7%;" />) для классов <img src="C:\Users\user\Desktop\Лекция 10\20210423(10).jpg" alt="20210423(10)" style="zoom:10%;" />

Наконец,

<img src="C:\Users\user\Desktop\Лекция 10\20210423(8).jpg" alt="20210423(8)" style="zoom:67%;" />

средние векторы для классов <img src="C:\Users\user\Desktop\Лекция 10\20210423(10).jpg" alt="20210423(10)" style="zoom:10%;" />

Искомые проекции векторов x на оптимальное направление w вычисляются как скалярное произведение:

<img src="C:\Users\user\Desktop\Лекция 10\20210423(9).jpg" alt="20210423(9)" style="zoom:50%;" />

Реализация на APL:

```
      ∇ W←Y fisher X;M1;M2
[1]     X←X-[2]M1←+⌿X÷≢X
[2]     Y←Y-[2]M2←+⌿Y÷≢Y
[3]     X←(⍉X)+.×X
[4]     Y←(⍉Y)+.×Y
[5]     W←(M1-M2)+.×⌹X+Y
      ∇
```

Левый аргумент У – матрица для ⍵2, если у нас 155 элементов в классе ⍵2 и размерность пространства признаков 10, т.е. 10 признаков, то матрица У 155х10, а правый аргумент Х – ⍵1, если у нас 205 элементов в классе ⍵1 и размерность пространства признаков 10, т.е. 10 признаков, то матрица Х 205х10.

При этом глубоко плевать какую мы матрицу привяжем к первому или второму классу.

1 и 2 строка: вычисляются средние векторы по каждому классу и происходит центрирование данных.

3 и 4: для первого и второго классов вычисляются ковариационные матрицы(S1 и S2).

5: доминой – обращение суммы ковариационных матриц перемножается на разность векторов средних, получаем W – направление.

Тестовые данные:

```
      x←?20 2⍴0
      y← ¯3 3+[2]?20 2⍴0
      plot ⊂[1]¨x y
```

матрица размером 20х2 для первого и второго классов из значений от 0 до 1

![20210423(C:\Users\user\Desktop\Лекция 10\20210423(12).png)](C:\Users\user\Desktop\Лекция 10\20210423(12).png)

Проектирование и гистограммы проекций:

```
      +w←x fisher y
¯1.065144169 0.9928572908
      9 hist2 (w+.×⍉x)(w+.×⍉y)
```

Вычисляем оптимальное направление

Строим гистограмму с девятью интервалами одну над другой(одна для первого класса другая для второго)

![20210423(C:\Users\user\Desktop\Лекция 10\20210423(13).png)](C:\Users\user\Desktop\Лекция 10\20210423(13).png)

Видим как замечательно отличаются классы между собой.

Для того, чтобы провести классификацию мы можем выбрать порог. Если проекция меньше 3, то ⍵1, если больше – ⍵2.

Иллюстрация процесса в двумерном пространстве:

```
⍝ making projection
      proj←{⍺×(⍵+.×⍺)÷⍺+.×⍺}   ⍝ функция для иллюстрации
      w proj y[1;]
 ̄3.044973696 2.838324073
 
⍝ apply with rank 1
      w proj⍤1⊢y[⍳2;]
¯3.044973696 2.838324073
¯2.832351409 2.640131571
 
⍝ prepare illustration on projections
      plot ⊂[1]¨x y  ⍝ рисуем наборы точек
      color 'blue'
      marker w proj⍤1⊢y   ⍝ проецируем их на оптимальную ось W для второго класса и проецируем их на оптимальную ось W для первого класса
      color 'black'
      marker w proj⍤1⊢x
      color 'gray'
      draw 0 0,[.5]w×5
```

![20210423(C:\Users\user\Desktop\Лекция 10\20210423(14).png)](C:\Users\user\Desktop\Лекция 10\20210423(14).png)

Примеры:

два класса(синенькие крестики и красненькие звездочки)

```
      a←(2/⍳10),[1.5]20⍴1 2
      b←(2/⍳10),[1.5]20⍴3 4
      0 plot ⊂[1]¨a b
```

![20210423(C:\Users\user\Desktop\Лекция 10\20210423(15).png)](C:\Users\user\Desktop\Лекция 10\20210423(15).png)

надо найти оптимальное направление Фишера, который максимизирует одномерный критерий Фишера. 

Данные заполнены в матрицах a и b. Длина вектора (параллельного оси ординат) нам не нужна. Спроектировали оба класса на этот вектор и получили гистограмму. Порог = 0,5

![20210423(C:\Users\user\Desktop\Лекция 10\20210423(16).png)](C:\Users\user\Desktop\Лекция 10\20210423(16).png)

Можем эти данные вращать разными матрицами. График преобразуется. Фишер строит оси проекций с направлением право-вниз(135*С). Порог = 0,5

![20210423(C:\Users\user\Desktop\Лекция 10\20210423(17).png)](C:\Users\user\Desktop\Лекция 10\20210423(17).png)

![20210423(C:\Users\user\Desktop\Лекция 10\20210423(18).png)](C:\Users\user\Desktop\Лекция 10\20210423(18).png)

Еще раз крутанем. Получим другое распределение. Фишер строит оси проекций с направление вправо-вверх(45*С). Порог = 0,5

![20210423(C:\Users\user\Desktop\Лекция 10\20210423(19).png)](C:\Users\user\Desktop\Лекция 10\20210423(19).png)

![20210423(C:\Users\user\Desktop\Лекция 10\20210423(20).png)](C:\Users\user\Desktop\Лекция 10\20210423(20).png)

Посмотрим, как себя поведут многомерные примерчики в шумовой диагностике.

```
      n←1 1 2 3 4 5 4 3 2 1 1 2 3 2 1
1
      c←1 1 2 3 4 5 4 3 2 1 1 1 1 1 1
1
      ⍴c
16
      plotn (⍳16) n c
```

n – для нормального распределения, c – для аномального распределения

нарисуем как выглядят средние спектры.

![20210423(C:\Users\user\Desktop\Лекция 10\20210423(21).png)](C:\Users\user\Desktop\Лекция 10\20210423(21).png)

на низких частотах при норме и кризисе совпадают спектры, тут мы ничего не отличим, а на высоких частотах видим, что при нормах у нас отсутствует составляющая спектра, а при кризисе(аномалии) в районе 13 появился пик(шум).

![20210423(C:\Users\user\Desktop\Лекция 10\20210423(2).jpg)](C:\Users\user\Desktop\Лекция 10\20210423(2).jpg)

![20210423(C:\Users\user\Desktop\Лекция 10\20210423(3).jpg)](C:\Users\user\Desktop\Лекция 10\20210423(3).jpg)

Но, если бы мы отличали только по двум точкам. Когда проводим больше двух измерений, картина может существенно измениться.

![20210423(C:\Users\user\Desktop\Лекция 10\20210423(1).jpg)](C:\Users\user\Desktop\Лекция 10\20210423(1).jpg)

Всегда должна быть обучающая выборка большого объема, чем больше, тем лучше.

```
      a←20 16⍴n
      b←23 16⍴c
      a1←a+.1×¯2+?(⍴a)⍴3
      b1←b+.1×¯2+?(⍴b)⍴3
      w←a1 Fisher b1
      plot w
```

Пример из курсовой:

Речь о диагностировании кризиса теплообмена по характеристикам акустического шума, которые поступают с ПЭ датчика. Размерность пространства 200. Спектр измерялся на 200  разных частотах. Грубо сократим число признаков до 10, но разрешение спектра ухудшится по частоте, но не смертельно.

```
      ⍴a←+/19 10 20⍴a
19 10
      ⍴b←+/21 10 20⍴b
21 10
      w←a Fisher b
      w
0.8478593433 1.377880251 0.7465740197 1.006157679 0.9072175876 ¯1.219360386
0.4514475061 1.120955995 1.471503214 0.63226001
      31 hist2 (w+.×⍉a)(w+.×⍉b)
```

Посчитаем компоненты весов для оптимальных проекций Фишера.

спроектируем нормальные и кризисные спектры на это направление в 10-ти мерном пространстве

справа норма, слева кризис теплообмена. Порог = 45 и с большим запасом сможем различать наши классы.

![20210423(C:\Users\user\Desktop\Лекция 10\20210423(22).png)](C:\Users\user\Desktop\Лекция 10\20210423(22).png)

# **Случайный поиск с адаптацией** 

***dp*** лежит в ***(0;1)*** – поправка вероятности выбора признака. 

Что делает алгоритм? Он задает равномерное распределение вероятности выбора признака. В данном случае вероятность для каждого признака 0,01.

![20210423(C:\Users\user\Desktop\Лекция 10\20210423(4).jpg)](C:\Users\user\Desktop\Лекция 10\20210423(4).jpg)

Чтобы определится с **n**, каждый раз проверяем оптимальный набор признаков на качество классификации и если нас все устраивает, то выбираем минимальное, возможное число признаков.

Т.е. мы 10 раз выбрали случайный набор из 100 объемом 5. Для каждого из r наборов посчитали критерий качества (какой хотим), например, для каждой 5-ки признаков будем искать оптимальное направление Фишера, проецировать на него и считать, какой одномерный критерий Фишера. Те наборы, для которых оптимальное направление дает наибольший одномерный критерий Фишера – лучше и наоборот. Из r выбираем наилучший, для лучшего набора увеличиваем вероятность выбора в последующем **p->p+dp** этого признака. Для наихудшего набора, т.е. уменьшаем вероятность последующего выбора худших признаков **p->p-dp**.

Так проводим R групп испытаний.

После Rxr испытаний выбираем наилучшую подсистему из n признаков. Она часто, но не всегда, оказывается в конце.

Как влияет dp?

Если в нашем примере dp=0,01, то признак, попавший в наихудшую группу, то вероятность его будет равна 0 и больше мы его не увидим. Конечно, это плохо(выбрали подсистему из 10-ти признаков 9 из которых отвратительные и один попался замечательный и мы этот случайный замечательный признак убьем)

dp=0, т.е. никакой адаптации нет, по ходу работы алгоритма никак не меняются вероятности выбора признаков, поэтому алгоритм сводится к случайному поиску.

Мы имеем некую обучающую выборку. Матрица **Х** с **n** признаками и вектор **Y**, где **yϵ[1,2]**, в данном случае рассматриваем классификацию на 2 класса. **n** – может быть очень большим.

Нужно выбрать **k** **<** **n** наиболее информативных(полезных) для классификации признаков.

Самый простой, надежный метод — это перебрать все возможные признаки, например:

```
      3!100
120
```

Чтобы выбрать лучшую тройку, нужен критерий, например, проекции многомерного Фишера и вычисление для этих проекций одномерного Фишера:

<img src="C:\Users\user\Desktop\Лекция 11\20210507.jpg" style="zoom:10%;" />

Чем больше F, тем подсистема полезней.

Найдя такую подсистему, будем использовать ее для алгоритма классификации.

Если из 10 получилось **i(the_best)=3,6,9** признаки, их спроектируем на плоскость и получим

<img src="C:\Users\user\Desktop\Лекция 11\20210507(2).jpg" style="zoom:20%;" />

Мы счастливы, никаких случайных поисков и адаптаций.

Если в задаче есть кризис теплообмена по спектрам акустического сигнала, т.е. установлен ПЭП и с него измеряем акустический сигнал и вычисляем спектральную плотность мощности(СПМ).

<img src="C:\Users\user\Desktop\Лекция 11\20210507(3).jpg" style="zoom:20%;" />

Простое представление:

<img src="C:\Users\user\Desktop\Лекция 11\20210507(4).jpg" style="zoom:20%;" />

Совершенно нормальным является оценивание СПМ на 200 частотах в диапазоне нахождения сигнала(ссылаемся на матрицу **Х**):

<img src="C:\Users\user\Desktop\Лекция 11\20210507(5).jpg" style="zoom:20%;" />

<img src="C:\Users\user\Desktop\Лекция 11\20210507(6).jpg" style="zoom:20%;" />

Это плохо.

Нам всегда нужно выбрать из имеющихся признаков информативные. Решаем две задачи, когда выбираем подмножество:

1. Облегчить вычисление, легче обращать матрицу 10 на 10, чем 200 на 200.
2. Убрать мешающие признаки, которые нам не позволяют сделать правильную классификацию.

Ищем 10 наилучших признаков из 200 – это 2 на 10^16 всевозможных комбинаций

```
      10!200
2.245100431Е16
```

Это очень большое число, которое мы будем перебирать всю жизнь. Именно для такой ситуации у нас есть случайной поиск с адаптацией, когда мы должны не 2 на 10^16 перебирать, а наши **r*R**-испытаний, где **r**-число испытаний в группе, **R**-число групп испытаний.

Начинаем с равномерного распределения вероятности выбрать каждый признак.

<img src="C:\Users\user\Desktop\Лекция 11\20210507(7).jpg" style="zoom:25%;" />

После проведения **r** испытаний по **k** признакам **(k < n)**, поощряем попавшие в наилучшую группу **p->p+dp**, наказываем попавших в наихудшую группу **p->p-dp**.

Наилучшая или наихудшая группа по выбранному критерию качество, например, многомерный Фишер.

1. Повторяем **r** испытаний с новым распределением вероятности выбора каждого признака.
2. Наказываем и поощряем  
3. Повторяем 1 и 2 **R** раз.
4. Выбираем из **r*R** – систему признаков.

По поводу параметров алгоритма:

1) **k**-сколько признаков хотим(чем меньше, тем лучше, когда их 2 – это шикарно)

```
      2!200
19900
```

2) **dp**-поощрение/наказание

3) **r**-число испытаний в группе

4) **R**-число групп испытаний

**1)**

Основным методом исследования в Date Science

<img src="C:\Users\user\Desktop\Лекция 11\20210507(8).jpg" style="zoom:15%;" />

Для решения каждой задачи в анализе данных существует множество методов.

Почему их много? Почему нет одного? Потому что очень сильно зависит от характеристик задачи того или иного метода. Задача, например, научить ПК отличать крестики от кружочков.

<img src="C:\Users\user\Desktop\Лекция 11\20210507(9).jpg" style="zoom:15%;" />

Если перепробуем все методы, то они все ее решат.

Если мы не знаем, кто здоровый или больной, а задача стоит в анализе имеющихся данных на однородность. Любой из 100 алгоритмов кластерного анализа, разобьет эти данные на 2 кластера. Проблема в том, что данные очень-очень разные в зависимости от разных встающих перед нами задач. И когда было всего 5 алгоритмов кластерного анализа, Иванов перепробовал их, но ему ни один не понравился.

<img src="C:\Users\user\Desktop\Лекция 11\20210507(10).jpg" style="zoom:15%;" />

Если Иванов бестолковый, то пойдет плакать в уголок, а когда у него будут спрашивать причину, он ответит: «кластерный анализ не работает на моих данных». Если он не бестолковый, то придумает 6 алгоритм кластерного анализа на его данных отработает, напишет статью, а Петров, Сидоров и другие прочитают эту статью и особенности их данных окажутся такими, как у Иванова, то они применят алгоритм Иванова и его включат в книжку к другим. Подобная история с методами классификации.

**2)**

**dp, max=1/n**, т.к. имеем начальное распределение с вероятностью выбора каждого признака **1/n**, то **> 1/n** не можем взять, т.к. отрицательных вероятностей у нас не бывает, поэтому max, который мы можем взять это **=1/n**.

Чем это плохо? Тем что можем убить хороший признак.

**dp, min=0**, долго-долго нет никакой адаптации, только случайный поиск. Т.к. мы не наказываем и не поощряем (отнимать или прибавлять 0, не имеет никакой полезности в информации), то адаптация исчезает, остается случайный поиск.

**3)**

**r, min=2**, попали оба плохих – несправедливо поощрю чуть менее плохого, попали оба хороших-несправедливо накажу чуть менее хорошего. Т.е. выбор хороших и плохих всего лишь по 2-м признакам:

<img src="C:\Users\user\Desktop\Лекция 11\20210507(11).jpg" style="zoom:20%;" />

вероятность попасть в хорошие или плохие – велика.

**r, max=нет!** Чем больше, тем больше вычислений.

**r, optimal=20**, принимаем на веру.

Например, взяли r=10

<img src="C:\Users\user\Desktop\Лекция 11\20210507(12).jpg" style="zoom:20%;" />

<img src="C:\Users\user\Desktop\Лекция 11\20210507(13).jpg" style="zoom:20%;" />

А вероятность, что они будут размазанные – высокая.

**4)**

Происходит адаптация. Чем больше **R**, тем лучше. Мы стремимся к наилучшей системе из **k** признаков.

Например, R=3000, т.е. должны провести 20*3000=60000 измерений.

Проходим по нашему **r*R**. Выбрали наилучшую систему из **k** признаков. 

SSA –sporadic search adaptation

```
best<-k SSA x1 x2       ⍝ x1 и x2 –матрицы для классов ω1 и ω2
```

Если **k** было взято 5 и **n** было 100, то **best** может выглядеть так **best: 11 93 5 44 59**, т.к. мы выбирали из 100 признаков случайно, то не обязана быть наилучшая группа упорядоченной.

Этот набор признаков нам обеспечит 90% точность и начинаем его применять в разных отраслях жизнедеятельности.

А чего, применяя SSA, полезного не сохранили как результат, кроме лучшего набора признаков. Что еще полезного после **R** шагов программы? Распределение вероятности выбора признаков (**p**)!!!

Если хотим получить лучший результат, придется всю работу начинать сначала. Но если в SSA засунуть уже выявленное лучшее распределение признаков, то будем выбирать уже из этого распределения и его будем поправлять и получится продолжение, а не начинание сначала.

# **Распознавание образов или классификация**

![JTMDbVDG4hU](C:\Users\user\Desktop\Лекция 12\JTMDbVDG4hU.jpg)Y Є [1,2,…,k], k-число классов. Y представлен в номинальной шкале.

Ищем ф-цию Y=f(x) это представление какого-то i-больного или i-образца. Имея обучающую выборку, т.е. набор объектов, которые подлежат распознаванию с известной принадлежностью каждого объекта классу какому-то.

Мы пытаемся построить ф-цию f(x), которая будет выдавать нам номер класса на выходе. Для того, чтобы научиться на обучающей выборке, построим такую ф-цию f(x), т.е. программируем и шлем куда-то.

Одна из классических задач распознавая образа – это распознавание рукописных цифр.

![Asu59acqFLk](C:\Users\user\Desktop\Лекция 12\Asu59acqFLk.jpg)

Если через какой-то пиксель прошло то, что написал клиент, то получаем 1, если пиксель в пустом поле, то будет 0.

Когда распознаем цифры у нас десять классов.

Наиболее сложная задача – распознавание текстов рукописных. Но и эта задача успешно решается. Если распознаем слова из латинских букв, то получаем 26 классов.

Методов распознавания куда и они продолжают появляться.

<img src="C:\Users\user\Desktop\Лекция 12\Метод эталонов.png" alt="Метод эталонов" style="zoom:150%;" />

Эталон - среднее по всем точкам каждого класса (зеленая точка на рисунке)

Среднее по всем классам

![yhXI_lJMWuc](C:\Users\user\Desktop\Лекция 12\yhXI_lJMWuc.jpg)

т.е. взяли все точки принадлежащие первому классу, например их 7, сложили их (сложить все первые и вторые координаты) и делим на количество точек.

ω_1: n_1=7

ω_2: n_2=9

ω_3: n_3=8

Только по этим точечкам мы производим суммирование и делим. Например, у нас упорядоченные вектора, которые относятся к каждому классу, т.е.

![maHy1j6dWdk](C:\Users\user\Desktop\Лекция 12\maHy1j6dWdk.jpg)

Теорема Пифагора для расстояний

![sQWoymcLWvg](C:\Users\user\Desktop\Лекция 12\sQWoymcLWvg.jpg)

Эталонами не удовлетворимся, т.к. у них есть слабые места:

![qoDwrpp83bc](C:\Users\user\Desktop\Лекция 12\qoDwrpp83bc.jpg)

Если у нас d_2>d_1, и точечку мы отнесем по рисунку к ω2, то получим ошибку, т.к. она должна быть в классе до центра которого от нее меньшее расстояние, т.е. ω1.

# Алгоритм Кендалла на APL

##### Описание алгоритма

Даны матрица «объект-свойство» X, размера MxN и вектор классификации С, размера M.

C[I]=1, если I-ый объект X[I;] принадлежит классу 1 и

C[I]=2, если I-ый объект X[I;] принадлежит классу 2

##### ОБУЧЕНИЕ:

для всех признаков xi, i=1,2,...N

определить пороги «чистого отсечения» p1 и p2, такие, что

x≤p1 ==> x принадлежит одному классу

x≥p2 ==> x принадлежит другому классу

отсортировать признаки по информативности (числу точек в области перечечения)

запомнить результаты

##### КЛАССИФИКАЦИЯ:

если значение признака 1 попадает в “чистую” область выдать диагноз и прекратить выполнение алгоритма иначе перейти к следующему признаку и повторить классификацию

если все признаки перебраны выдать диагноз “неизвестный класс” и прекратить выполнение алгоритма.

### Функции

Число точек в области пересечения множеств (выборок)

Аргументы a ,b - выборки значений признака (векторы) для классов 1,2.

Результат n - число точек в области пересечения

```
      ∇ n←a cross b
[1]  n←+/b<⌈/a25
[2]  n←n⌊+/b>⌊/a25          ⍝сколько b>min a
[3]  ∇
```

Пример:

```
      1 2 3 cross 2.1 2.9 4 5
2      
```

### Правило для одного признака

Аргументы a,b - выборки значений признака (векторы) для классов 1,2.

Результат r — вектор 4-х элементов: порог ≤, класс для этого порога, порог ≥, класс для этого порога

```
      ∇ r←a rule b;c;ab      ⍝множество значений признака для b (b;c;ab)
[1]  c←∊(⍴¨a b)⍴¨1 2
[2]  c←c[⍋a,b]
[3]  ab←(a,b)[⍋a,b]
[4]  r←(⌈/(∧\c=↑c)/ab),↑c
[5]  r←r,(⌊/(⌽∧\( ¯1↑c)=⌽c)/ab), ¯1↑c
[6]  ∇
```

Пример:

```
      1 2 3 rule 2.1 2.9 4 5
2 1 4 2
```

что означает:

если x≤2 то класс 1

если x≥4 то класс 2

### Набор всех правил

Аргументы a,b — векторы векторов значений всех признаков для класса 1 (a) и для класса 2 (b)

Результат r — вектор векторов всех правил с номерами признаков, признаки отсортированы по информативности (числу точек в области пересечения).

```
      ∇ r←a rules b;i
[1]  i←⍋a cross¨b
[2]  r←a rule¨b
[3]  r←(⍳⍴a),¨r
[4]  r←r[i]
[5]  ∇
```

Пример:

```
      (1 2 3)(9 4 8) rules (2.1 2.9 4 5)(3 4.5 1 2)
2 3 2 8 1 1 2 1 4 2
      disp (1 2 3)(9 4 8) rules (2.1 2.9 4 5)(3 4.5 1 2)
.→------------------------.
| .→--------. .→--------. |
| |2 3 2 8 1| |1 2 1 4 2| |
| '~--------' '~--------' |
'∊------------------------'
```

слева: x<3 - класс 2; x>8 - класс 1          справа: x<2 - класс 1; x>4 - класс 2

### Классификация по одному правилу

Аргумент (правый) x — точка подлежащая классификации

Аргумент (левый) r — правило в формате функции rule

Результат — номер класса

```
       ∇ c←r class x
[1]  c←0
[2]  ⍎(x≤r[1])/'c←r[2]'
[3]  ⍎(x≥r[3])/'c←r[4]'
[4]  ∇
```

Пример:

```
      2 1 4 2 class 0
1
      2 1 4 2 class 5
2
      2 1 4 2 class 3 ⍝ do not know
0
```

### Диагностирование (классификация по всем правилам)

Аргумент (правый) x — точка подлежащая классификации

Аргумент (левый) rules — все правила в формате функции rules

Результат — номер класса

```
      ∇ d←rules diag x;i;r
[1]  i←1
[2]  L:r←i⊃rules
[3]  d←(1↓r)class x[↑r]
[4]  →(d≠0)/0
[5]  →((⍴rules)≤i←i+1)/L
[6]  ∇
```

Применение для выборки ирисов

```
ir1: 50x4 matrix of class 1 of Iris
ir2: 50x4 matrix of class 2 of Iris
ir3: 50x4 matrix of class 3 of Iris
```

Применение правил для классов 1 и 2

```
      rr←(⊂[1]ir1) rules ⊂[1]ir2
      disp rr
.→------------------------------------------------------------.
| .→----------. .→----------. .→------------. .→------------. |
| |3 1.9 1 3 2| |4 0.6 1 1 2| |1 4.9 1 5.8 2| |2 2.2 2 3.5 1| |
| '~----------' '~----------' '~------------' '~------------' |
'∊------------------------------------------------------------'
```

Проверка точности классификации

```
      +d←(⊂rr) diag ̈ ⊂[2]ir1
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
      d∧.=1
1
      100×(+/d=1)÷⍴d
100
      d←(⊂rr) diag ̈ ⊂[2]ir2
      100×(+/d=2)÷⍴d
100      
```

##### !!!признаки - спектральные амплитуды на заданных частотах

# Многомерный вероятностный метод классификации основанные на аппроксимации гауссовским распределением.



У нас есть **Nc**-число классов и для каждого класса мы оцениваем условную плотность распределения многомерную для векторочков **х**, при условии, что эти **х** принадлежат классу **⍵(i)**
Общая запись многомерного гауссовского распределения
![20210528_5](C:\Users\user\Desktop\Лекция14\20210528_5.jpg)
Почему это будет ковариационна матрица
Вот для двумерного случая, когда мы перемножаем эти две матрицы, то получаем 

<img src="C:\Users\user\Desktop\Лекция14\20210528_2.jpg" alt="20210528_2" style="zoom:15%;" />

￼<img src="C:\Users\user\Desktop\Лекция14\20210528_3.jpg" alt="20210528_3" style="zoom:15%;" />

Кто здесь **i** у нас этих векторов
￼<img src="C:\Users\user\Desktop\Лекция14\20210528_4.jpg" alt="20210528_4" style="zoom:15%;" />
Соответственно добавим индекс **i**
￼<img src="C:\Users\user\Desktop\Лекция14\20210528_9.jpg" alt="20210528_9" style="zoom:60%;" />
Далее

<img src="C:\Users\user\Desktop\Лекция14\20210528_6.jpg" alt="20210528_6" style="zoom:20%;" />

<img src="C:\Users\user\Desktop\Лекция14\20210528_7.jpg" alt="20210528_7" style="zoom:15%;" />

Коэффициент ковариации между 1 и 2 признаком. Т.е. наша матрица после суммирования и деления на  **n** выглядит следующим образом:

<img src="C:\Users\user\Desktop\Лекция14\20210528_8.jpg" alt="20210528_8" style="zoom:15%;" />

В многомерном случае, когда **n>2**

<img src="C:\Users\user\Desktop\Лекция14\20210528_10.jpg" alt="20210528_10" style="zoom:15%;" />

Т.к. ковариации не зависят от порядка признаков, то эта матрица симметричная.

Формула Байеса для условной вероятности того, что у нас наблюдается класс **⍵(i)** при условии, что померен вектор **x** - это произведение априорной вер-ти класса **⍵(i)** и на полную вероятность x при условии **⍵(i)** и делим на нормировочный множитель, который по всем классам пробегает.

<img src="C:\Users\user\Desktop\Лекция14\20210528_11.jpg" alt="20210528_11" style="zoom:15%;" />

Формула Байеса – гениальная вещь, потому что впрямую вероятность не можем оценить.

Оценить вероятность значит зафиксировать набор наших измерений, **х** -вектор из сахара в моче, верхнее давление и т.д., мы должны зафиксировать, что это равно тому-то и смотреть чем болен этот клиент. 

Например, болезнью **⍵1**, теперь ищем клиента с точно такими же анализами и посмотрим, чем болеет он, но мы никогда не найдем такого клиента, чтобы были анализы один в один с кем-то другим.

Собрав всех клиентов, которые болеют болезнью **⍵1** мы запросто можем оценить распределение показателей анализов, то же самое для **⍵2** и т.д.

Т.е. ту вероятность, которую мы не можем экспериментально получить, мы получаем благодаря Байесу.

Мы можем имея 3 класса посчитать для первого, второго и третьего классов формулу Байеса и выбрать тот диагноз, вероятность которого больше всего.

Но, чтобы это все не городить, т.к. мы выбираем максимум, переходим к решающим ф-циям, которые приведут нас к тому же решению, что и по полному распределению, но более простому в вычислительном плане.

Первое, что мы делаем при виде экспоненты – прологарифмируем.

![20210528_12](C:\Users\user\Desktop\Лекция14\20210528_12.jpg)

Для сравнения этих вероятностей один и то же знаменатель, поэтому он не важен, максимум останется прежним. Логарифмируем только числитель.

Общий случай, когда ковариационная матрица своя для каждого класса, тогда у нас функция будет решающая – квадратичная, т.е. наши измерения для какого-то объекта, который мы классифицируем, они будут входить во второй степени после перемножения этих матриц.

Если хотим один класс отличить от другого нам нужен **d1(х)** **d2(x)** и ищем максимум. Границей будет разность этих решающих функций приравненная к **0**.

![20210528_13](C:\Users\user\Desktop\Лекция14\20210528_13.jpg)

*Как упрощается при разных предположениях наша решающая функция?*

Одно из упрощений – принимаем, что ковариационная матрица одинакова для всех классов.

<img src="C:\Users\user\Desktop\Лекция14\20210528_14.jpg" alt="20210528_14" style="zoom:20%;" />

Ковариационная матрица в двумерном случае – это срез на какой-то высоте холмика распределения.

<img src="C:\Users\user\Desktop\Лекция14\20210528_15.jpg" alt="20210528_15" style="zoom:20%;" />

если мы договоримся о одном и том же ур-не среза, то с меньшими главными сторонами эллипс будет говорить, что для этого класса у нас менее сильная корреляция с такими признаками, а с большими главными сторонами, наоборот и наклон эллипса говорит, что для **⍵1** у нас сильна корреляция **х2** и **х1** и она положительная. для класса **⍵2** сильная корреляция, но отрицательная и для класса **⍵3** корреляция отсутствует, но дисперсия **х1** много больше, чем дисперсия **х2**.

Если мы предположим, что все **С** равны , то видим, что эллипсы отличаются только средним значением.

<img src="C:\Users\user\Desktop\Лекция14\20210528_16.jpg" alt="20210528_16" style="zoom:20%;" />

Направленность, которую указывает главная оси между **x1** и **x2** у всех одинаковая и размеры тоже.

В этом случае логарифм будет одинаков для всех **i**, т.к. ищем максимум, то определитель ковариационной матрицы мы выкидываем, а скобочки мы раскрываем, приводим подобные и в итоге получаем:

<img src="C:\Users\user\Desktop\Лекция14\20210528_17.jpg" alt="20210528_17" style="zoom:15%;" />

Где<img src="C:\Users\user\Desktop\Лекция14\20210528_18.jpg" alt="20210528_18" style="zoom:5%;" />  не включает **x**, это некая константа для каждого класса, а<img src="C:\Users\user\Desktop\Лекция14\20210528_19.jpg" alt="20210528_19" style="zoom:5%;" />  включает **x**, когда мы умножим обратную ковариационную матрицу на  мат.ожидание, то получим некий числовой вектор и когда мы умножим это вектор столбец на этот вектор строку, т.к. транспонированный, то получим сумму:

<img src="C:\Users\user\Desktop\Лекция14\20210528_20.jpg" alt="20210528_20" style="zoom:15%;" />

Это тоже некая константа.

Т.е. решающая функция линейно зависит от **x**, если возьмем разность для двух классов решающих функций и приравняем ее к нулю(к границе), то в данному случае получим выражение, которое показывает, что эта граница является линейной относительно **x**.

Поэтому, если имеем неравные ковариационные матрицы, то у нас она квадратичная относительно **x**.

<img src="C:\Users\user\Desktop\Лекция14\20210528_21.jpg" alt="20210528_21" style="zoom:15%;" />

А если ковариационные матрицы равны, то она будет для двумерного случая прямой, для трехмерного – плоскостью, для **n**-размерности – гиперплоскостью, но все это всего лишь линейная функция от всех **x**.

<img src="C:\Users\user\Desktop\Лекция14\20210528_22.jpg" alt="20210528_22" style="zoom:15%;" />

Следующий шаг по упрощению. это предположение, что наша ковариационные матрицы не только одинаковы, но и диагональные, т.е. ковариации **i** и **j =0**.

Запись для решающей ф-ции

<img src="C:\Users\user\Desktop\Лекция14\20210528_23.jpg" alt="20210528_23" style="zoom:15%;" />


Она означает, что у нас входит скалярное произведение вектора на мат.ожидание **i**-ого класса и диаметрически это будет

<img src="C:\Users\user\Desktop\Лекция14\20210528_24.jpg" alt="20210528_24" style="zoom:15%;" />

Получается случай эталонов.

Псмотрим как работает на примере диагностирования акустических шумов:

![20210528_25](C:\Users\user\Desktop\Лекция14\20210528_25.jpg)

*правый аргумент матрица спектров акустического шума

*левый аргумент, это вектор из 1 и 2, где 1-нормальный теплообмен, а 2-кризис теплообмена

Получили в результате обучения вектор размерности два, т.е. вектор векторов и  если посмотрим на размерность каждого элемента, то получим 4, от того, что нужно знать априорную вероятность, определитель ковариационной матрицы, мат.ожидание для каждого класса и обратную ковариационную матрицу.

Проводим классификацию этих спектров. Сравниваем по байесовскому многомерному правилу с физической классификацией, которую мы знаем(кризис или норма).

Для первого случая упрощения мы снова проводим обучение.

Для каждого класса после обучения мы имеем не 4, а 3 параметра, уже не нужен определитель ковариационной матрицы, потому что он у всех одинаковый.

Если эту классификацию сравним с истиной , то они не совпадают и когда мы считаем сколько у нас ошибок, то одна ошибка появляется от нашего предположения.

*Диагональная матрица*

К тем же данным ее применяем, по ним же проводим классификацию, классификация не совпадает с фактическим выражением.

#### KNN – k ближайших соседей

Он очень простой, индифферентный к размерности признаков, замечательный алгоритм.

![20210528_26](C:\Users\user\Desktop\Лекция14\20210528_26.jpg)

Не имеет значения сколько классов. **k**-параметр алгоритма. 

Пусть k=9, мы находим самого ближайшего соседа к точке с неизвестной классификацией, второго, третьего, четвертого,….,девятого ближайшего соседа и проводим круг с центром с точкой неизвестной принадлежности и радиус для k-ого ближайшего соседа.

Смотрим, что в круг попались два крестика, три треугольника и четыре кружочка. 

К какому классу отнести неизвестную точку? К тем, кого больше, т.е. к кружкам.

*Прелесть этого алгоритма*. Если трехмерное пространство признаков, нам все равно, мы опять для неизвестной точки находим расстояния до всех точек обучающей выборки, находи **k**-ую ближайшую и проводим через нее шар и считаем сколько элементов каждого класса вошло в него, далее по большинству эту точку классифицируем.

Если у нас 400-мерное пространство, ничего страшного.

<img src="C:\Users\user\Desktop\Лекция14\20210528_261.jpg" alt="20210528_261" style="zoom:15%;" />

Считаем расстояние от точки **x** до всех **y**, располагаем их в порядке возрастания, находим **k**-ую и для  <img src="C:\Users\user\Desktop\Лекция14\20210528_27.jpg" alt="20210528_27" style="zoom:5%;" />подсчитываем сколько элементов каждого класса и выбиваем по максимальному представительству.

Еще одна прелесть в том, что он работает не только для задач классификации, но и для регрессии.

Когда хотим в какой-то точке оценить **y**, если k=3, то ищем три ближайшие точки, применяем усреднение(медиана, взвешенное и т.п.)

<img src="C:\Users\user\Desktop\Лекция14\20210528_28.jpg" alt="20210528_28" style="zoom:15%;" />

В методе KNN регрессии, как и в KNN классификации все равно на размерность пространства. 

Трехмерное пространство:

<img src="C:\Users\user\Desktop\Лекция14\20210528_29.jpg" alt="20210528_29" style="zoom:15%;" />

Он тоже дает одну ошибку. Алгоритм **k** берется из оптимизации.

#### Метод потенциальных ф-ций.

Подразумевается разделение на 2 класса и он основан на том, что мы к каждой точке одного класса приписываем 1-ый положительный заряд, к каждой точке второго класса приписываем 1-ый отрицательный заряд и для неизвестной точке считаем какой потенциал, если он положительный, то относим к первому классу, если отрицательный – ко второму классу

![20210528_29](C:\Users\user\Desktop\Лекция14\20210528_29.png)

Для одномерного случая:

<img src="C:\Users\user\Desktop\Лекция14\20210528_30.jpg" alt="20210528_30" style="zoom:15%;" />
