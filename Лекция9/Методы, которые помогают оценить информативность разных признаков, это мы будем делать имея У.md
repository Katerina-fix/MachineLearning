# **Методы, которые помогают оценить информативность разных признаков, это мы будем делать имея У.**

Х – матрица, объект/свойство

У – в номинальной шкале название класса или номер класса, если в непрерывной шкале, то это переменная, которая зависит от наших признаков(Х)

![](/home/ekaterina/Desktop/AO/20210416/0416.png)

*Будем говорить о двух классах*

т.е. ω_к указывает на то, какой класс имеет место быть (1 или 2). И наша задача научиться классифицировать, т.е. построить такую ф-цию y=f(х), которая нам позволяет предсказать на каких-то новых данных, например, в медицинской диагностике, у-здоровый больной, то для какого-то Сидорова мы опишем набор его анализов вектором Х, подставим в эту ф-цию, которую мы обучили, используя нашу обучающую выборку и получим у=1 – здоров, у=2 – болен тем-то.

Существует масса методов (линейных/нелинейных) построения этой функции. Одни методы могут быть очень простые и идеалогические с вычислительной точки зрения, но оно и работает часто фигово. Другие методы, даже, если с точки зрения понимания нет ничего особо трудного, что именно делается, а трудность для вычислений есть и в частности, она из гаденьких задач вычислительной математики и линейной алгебры, на которых основаны большинство методов распознавания образов классификаций, построение матриц функций f, это обращение матриц.

Пока у нас какие-то детские(маленький размер) матрицы. Если возьмем все имеющиеся признаки и начнем со всеми сразу строить ф-цию f, то придется обращать матрицу 100х100 или 1000х1000, не дай Бог, потому что признаков у нас может быть бесконечное множество.

Методы распознавания образов классификаций сильно зависит от размерности. Здесь стоит задача понижения размерности пространства признаков (не обязательно до n=2!!!, но до числа n облегчающего построение ф-ции f-распознавания(классификации)).

Т.е. мы не собираемся ничего визуализировать, но хотим посмотреть как наши признаки отличаются по информативности с т.з. распознавания одного от другого и выбрать какое-то небольшое (от 100 выберем, например, 10).

### **Критерий информативности (полезности) признаков для распознавания**

Есть какой-то признак Хi – i столбец нашей матрицы Х. Заглядываем в вектор У, берем сначала все значения Х, когда был первый класс и строим по ним гистограмму (ω1-класс). По тому же Хi берем значения, соответствующие классу ω2 и стоим еще одну гистограмму.

![0416(1)](/home/ekaterina/Desktop/AO/20210416/0416(1).png)

То же самое делаем для признака Хj

![0416(2)](/home/ekaterina/Desktop/AO/20210416/0416(2).png)

Какой из признаков нам нравится больше? **Хj** !!!

Как мы можем эту хорошесть описать? Можем взять модуль разности m1 и m2 или возвести эту разность в квадрат. Т.е. мы можем оценить полезность разностью условных средних, беря ее в квадрат. 

![0416(3)](/home/ekaterina/Desktop/AO/20210416/0416(3).png)

Или от номера признака построить график и получить:

![0416(4)](/home/ekaterina/Desktop/AO/20210416/0416(4).png)

Можем сказать, что самые информативные признаки обозначены красными точками. Можем взять какой-то порог (посмотрим распределение (m1 - m2)^2, т.е. будет много признаков болтаться вокруг какого-то небольшого значения этого квадрата разности, а какая-то часть будет иметь большее чем у остальных квадрата разности средних, там и будет порог).

![0416(5)](/home/ekaterina/Desktop/AO/20210416/0416(5).png)

k Є множеству информативных признаков, если (m1^k - m2^k)^2 > порога. Неважно, сколько останется этих признаков, в функцию f подставляем не полный вектор Х, а Х_k Є множеству информативных признаков.

Сейчас нам не важно, какой метод мы будем использовать, возьмем самый примитивный – метод эталонов, когда в качестве эталона берем центры. Набор информативных признаков подаем и смотрим к какому центру ближе.

### **Возьмем еще два других признака**

Строим гистограммы, как делали это выше

![1](/home/ekaterina/Desktop/AO/20210416/1.jpg)

По нашему критерию признак j лучше, а по ощущению лучше i. Следовательно, мы сочинили плохой критерий!

Что мы не учли? **ИЗМЕНЧИВОСТЬ ВНУТРИ КЛАССОВ**. Т.е. для j-ого разность средних большая, но внутри значения класса меняются +/- лапоть. Соответственно, эти распределения сильно пересекаются и мы будем допускать много ошибок (точку для второго класса мы будем относить к первому и наоборот, так нельзя!)

![2](/home/ekaterina/Desktop/AO/20210416/2.jpg)

*Признак Хi имеет небольшую относительно j разность средних по классам, но внутри каждого класса они мало меняются, соответственно, **можем легко разделить два класса каким-то порогом***:

![3](/home/ekaterina/Desktop/AO/20210416/3.jpg)

**Кроме мер положения, должны использоваться меры изменчивости.**

Меры изменчивости:

- ​	среднеквадратичное отклонение
- ​	сумма квадратов
- ​	т.д.

![4](/home/ekaterina/Desktop/AO/20210416/4.jpg)

Чем S1^2 отличается от дисперсии отклонений по первому классу? Дисперсия делится на число точек в первом классе.

Мы не делим на число точек, потому что мы используем эту сумму и нас волнует общая изменчивость, если у нас представителей одного класса много, а другого – мало, то при делении на число точек они у нас будут мало отличаться. Сумма наших S будет отображать общую изменчивость в классах.

**F – критерий Фишера.**

По критерию Фишера Xi будет лучше Xj, потому огромное расстояние между средними поделится на огромную сумму квадратов отклонений по ω1 и ω2.

Например, у нас есть 100 исходных признаков, вычисляем значение Фишера для каждого из этих 100 признаков и строим или распределение значений критерия Фишера, или кривую, показывающую значение от номера признака, или и то и то вместе. На основе этих картинок выбираем небольшое число меньше исходных 100, например, 10 наиболее информативных по критерию Фишера. Дальше с ними работаем, строим функцию f для распознавания классификаций.

Звучит хорошо в случае, если у нас данные «чистые» (отсутствие ошибочных значений и выбросов), если взять любую книжку по науке о данных, но с разными названиями, они для нас эквивалентны.

Если данные «грязные», т.е. присутствуют выбросы.

Пусть изначально Fj>Fi, ошибочное значение или выбросы это

![5](/home/ekaterina/Desktop/AO/20210416/5.jpg)

К чему это приведет? m1 поедет в сторону выбросов, m2 в сторону другого выброса, таким образом разность средних у нас может оказаться меньше, чем для признака Хi и это в целом хороший признак, если выкинуть эти выбросы, то Хi будет хуже Хj.

Выбросить выбросы – значит использовать для вычисления мер положений робастные (устойчивые к выбросам и ошибкам в данных) методы оценивания мер положения.

А медиане наплевать на выбросы:

![10](/home/ekaterina/Desktop/AO/20210416/10.jpg)

Здесь мы не говорим на сколько точек больше, на сколько меньше, они могут хоть в Америку убежать, но для нас их две штуки и они попадут в те 50 больше/меньше медианы. Медиане наплевать, она со своего места никуда не сдвинется.

*Поэтому в робастном критерии Фишера мы напишем med1-med2 и возьмем модуль.*

Что делать с изменчивостью, которая тоже чувствительна к выбросам? **Нужно поставить порог.**

![6](/home/ekaterina/Desktop/AO/20210416/6.jpg)

Вместо суммы квадратов отклонений от среднего, глядя на ящик с усами, в знаменатель нужно поставить межквартильный размах для ω1 и ω2.

![11](/home/ekaterina/Desktop/AO/20210416/11.jpg)

![7](/home/ekaterina/Desktop/AO/20210416/7.jpg)

Можем посчитать для всех наших 100 признаков этот  робастный критерий Фишера. Построить его в гистограмму или кривульку с порогом, отобрать и использовать информативные признаки.

Всегда ли надо использовать робастный? НЕТ. Использовать нужно оба два. В идеале, если чистая выборка, то у нас значения будут лежать на прямой(более информативные признаки), информативные признаки по Фишеру будут лежать так же как и по робастному Фишеру

![8](/home/ekaterina/Desktop/AO/20210416/8.jpg)

Если вдруг какая-то точка по Фишеру хорошо, а по робастному она мало информативная, то значит есть выборки какие-то по этому признаку отклонения и в другую сторону тоже. Т.о. мы диагностируем проблемы в нашей выборке (какого черта Х25 сильно отличаются робастный и обычный критерии Фишера, смотрим распределение и видим выбросы, это нам помогает). 

![9](/home/ekaterina/Desktop/AO/20210416/9.jpg)

Если они похоже себя ведут, то мы выбрали множество информативных и с ним счастливо стоим функцию f.