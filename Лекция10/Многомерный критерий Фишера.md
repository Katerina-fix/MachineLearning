# **Многомерный критерий Фишера**

По математике он совпадает с линейным дискриминантным анализом.

Есть двумерное пространство, где распределены два класса. Признак Х1 плох для классификации распознавания этих классов, потому что проекции на этот признак из областей двух классов смешиваются, тоже самое и с признаком Х2.

Поищем направление проекции, на которое максимизирует одномерный критерий Фишера(такое направление, чтобы критерий Фишера был максимальным) и  можно будет отделить один класс от другого. 

Нужно провести направление f под углом 135*С. Рисуем проекции наших классов на это направление. Они не пересекаются.

Мы не делим на число точек, когда считаем вариацию, чтобы если в одном классе оказалось точек много больше, чем в другом, после деления на число точек у нас S1 и S2 уравняются в своих правах. А когда просуммируем квадраты отклонений от своего среднего, то этого не происходит.

Изобразим программку, которая с шагом в 1*С будет крутить эту ось f для проектирования, на каждом градусе определит проекции, определит критерий F и возьмет те градусы, которые соответствуют максимальному критерию F. Если мы будем в трехмерном пространстве. То придется выбирать точки на сфере и придется перебирать больше направлений.

К счастью, для линейного дискриминантного анализа  и для многомерного критерия Фишера существует простая математика.

![20210423](/home/ekaterina/Desktop/AO/Лекция 9/20210423.jpg)

Оптимальное направление w для проектирования векторов x в многомерном
пространстве признаков определяется как:

<img src="/home/ekaterina/Desktop/AO/Лекция 9/20210423(5).jpg" alt="20210423(5)" style="zoom:67%;" />

<img src="/home/ekaterina/Desktop/AO/Лекция 9/20210423(6).jpg" alt="20210423(6)" style="zoom:67%;" />

<img src="/home/ekaterina/Desktop/AO/Лекция 9/20210423(7).jpg" alt="20210423(7)" style="zoom: 67%;" />

i = 1, 2 −ковариационные матрицы (с точностью до множителя <img src="/home/ekaterina/Desktop/AO/Лекция 9/20210423(11).jpg" alt="20210423(11)" style="zoom: 7%;" />) для классов <img src="/home/ekaterina/Desktop/AO/Лекция 9/20210423(10).jpg" alt="20210423(10)" style="zoom:10%;" />

Наконец,

<img src="/home/ekaterina/Desktop/AO/Лекция 9/20210423(8).jpg" alt="20210423(8)" style="zoom:67%;" />

средние векторы для классов <img src="/home/ekaterina/Desktop/AO/Лекция 9/20210423(10).jpg" alt="20210423(10)" style="zoom:10%;" />

Искомые проекции векторов x на оптимальное направление w вычисляются как скалярное произведение:

<img src="/home/ekaterina/Desktop/AO/Лекция 9/20210423(9).jpg" alt="20210423(9)" style="zoom:50%;" />

Реализация на APL:

```
      ∇ W←Y fisher X;M1;M2
[1]     X←X-[2]M1←+⌿X÷≢X
[2]     Y←Y-[2]M2←+⌿Y÷≢Y
[3]     X←(⍉X)+.×X
[4]     Y←(⍉Y)+.×Y
[5]     W←(M1-M2)+.×⌹X+Y
      ∇
```

Левый аргумент У – матрица для ⍵2, если у нас 155 элементов в классе ⍵2 и размерность пространства признаков 10, т.е. 10 признаков, то матрица У 155х10, а правый аргумент Х – ⍵1, если у нас 205 элементов в классе ⍵1 и размерность пространства признаков 10, т.е. 10 признаков, то матрица Х 205х10.

При этом глубоко плевать какую мы матрицу привяжем к первому или второму классу.

1 и 2 строка: вычисляются средние векторы по каждому классу и происходит центрирование данных.

3 и 4: для первого и второго классов вычисляются ковариационные матрицы(S1 и S2).

5: доминой – обращение суммы ковариационных матриц перемножается на разность векторов средних, получаем W – направление.

Тестовые данные:

```
      x←?20 2⍴0
      y← ¯3 3+[2]?20 2⍴0
      plot ⊂[1]¨x y
```

матрица размером 20х2 для первого и второго классов из значений от 0 до 1

![20210423(12)](/home/ekaterina/Desktop/AO/Лекция 9/20210423(12).png)

Проектирование и гистограммы проекций:

```
      +w←x fisher y
¯1.065144169 0.9928572908
      9 hist2 (w+.×⍉x)(w+.×⍉y)
```

Вычисляем оптимальное направление

Строим гистограмму с девятью интервалами одну над другой(одна для первого класса другая для второго)

![20210423(13)](/home/ekaterina/Desktop/AO/Лекция 9/20210423(13).png)

Видим как замечательно отличаются классы между собой.

Для того, чтобы провести классификацию мы можем выбрать порог. Если проекция меньше 3, то ⍵1, если больше – ⍵2.

Иллюстрация процесса в двумерном пространстве:

```
⍝ making projection
      proj←{⍺×(⍵+.×⍺)÷⍺+.×⍺}   ⍝ функция для иллюстрации
      w proj y[1;]
 ̄3.044973696 2.838324073
 
⍝ apply with rank 1
      w proj⍤1⊢y[⍳2;]
¯3.044973696 2.838324073
¯2.832351409 2.640131571
 
⍝ prepare illustration on projections
      plot ⊂[1]¨x y  ⍝ рисуем наборы точек
      color 'blue'
      marker w proj⍤1⊢y   ⍝ проецируем их на оптимальную ось W для второго класса и проецируем их на оптимальную ось W для первого класса
      color 'black'
      marker w proj⍤1⊢x
      color 'gray'
      draw 0 0,[.5]w×5
```

![20210423(14)](/home/ekaterina/Desktop/AO/Лекция 9/20210423(14).png)

Примеры:

два класса(синенькие крестики и красненькие звездочки)

```
      a←(2/⍳10),[1.5]20⍴1 2
      b←(2/⍳10),[1.5]20⍴3 4
      0 plot ⊂[1]¨a b
```

![20210423(15)](/home/ekaterina/Desktop/AO/Лекция 9/20210423(15).png)

надо найти оптимальное направление Фишера, который максимизирует одномерный критерий Фишера. 

Данные заполнены в матрицах a и b. Длина вектора (параллельного оси ординат) нам не нужна. Спроектировали оба класса на этот вектор и получили гистограмму. Порог = 0,5

![20210423(16)](/home/ekaterina/Desktop/AO/Лекция 9/20210423(16).png)

Можем эти данные вращать разными матрицами. График преобразуется. Фишер строит оси проекций с направлением право-вниз(135*С). Порог = 0,5

![20210423(17)](/home/ekaterina/Desktop/AO/Лекция 9/20210423(17).png)

![20210423(18)](/home/ekaterina/Desktop/AO/Лекция 9/20210423(18).png)

Еще раз крутанем. Получим другое распределение. Фишер строит оси проекций с направление вправо-вверх(45*С). Порог = 0,5

![20210423(19)](/home/ekaterina/Desktop/AO/Лекция 9/20210423(19).png)

![20210423(20)](/home/ekaterina/Desktop/AO/Лекция 9/20210423(20).png)

Посмотрим, как себя поведут многомерные примерчики в шумовой диагностике.

```
      n←1 1 2 3 4 5 4 3 2 1 1 2 3 2 1
1
      c←1 1 2 3 4 5 4 3 2 1 1 1 1 1 1
1
      ⍴c
16
      plotn (⍳16) n c
```

n – для нормального распределения, c – для аномального распределения

нарисуем как выглядят средние спектры.

![20210423(21)](/home/ekaterina/Desktop/AO/Лекция 9/20210423(21).png)

на низких частотах при норме и кризисе совпадают спектры, тут мы ничего не отличим, а на высоких частотах видим, что при нормах у нас отсутствует составляющая спектра, а при кризисе(аномалии) в районе 13 появился пик(шум).

![20210423(2)](/home/ekaterina/Desktop/AO/Лекция 9/20210423(2).jpg)

![20210423(3)](/home/ekaterina/Desktop/AO/Лекция 9/20210423(3).jpg)

Но, если бы мы отличали только по двум точкам. Когда проводим больше двух измерений, картина может существенно измениться.

![20210423(1)](/home/ekaterina/Desktop/AO/Лекция 9/20210423(1).jpg)

Всегда должна быть обучающая выборка большого объема, чем больше, тем лучше.

```
      a←20 16⍴n
      b←23 16⍴c
      a1←a+.1×¯2+?(⍴a)⍴3
      b1←b+.1×¯2+?(⍴b)⍴3
      w←a1 Fisher b1
      plot w
```

Пример из курсовой:

Речь о диагностировании кризиса теплообмена по характеристикам акустического шума, которые поступают с ПЭ датчика. Размерность пространства 200. Спектр измерялся на 200  разных частотах. Грубо сократим число признаков до 10, но разрешение спектра ухудшится по частоте, но не смертельно.

```
      ⍴a←+/19 10 20⍴a
19 10
      ⍴b←+/21 10 20⍴b
21 10
      w←a Fisher b
      w
0.8478593433 1.377880251 0.7465740197 1.006157679 0.9072175876 ¯1.219360386
0.4514475061 1.120955995 1.471503214 0.63226001
      31 hist2 (w+.×⍉a)(w+.×⍉b)
```

Посчитаем компоненты весов для оптимальных проекций Фишера.

спроектируем нормальные и кризисные спектры на это направление в 10-ти мерном пространстве

справа норма, слева кризис теплообмена. Порог = 45 и с большим запасом сможем различать наши классы.

![20210423(22)](/home/ekaterina/Desktop/AO/Лекция 9/20210423(22).png)

# **Случайный поиск с адаптацией** 

***dp*** лежит в ***(0;1)*** – поправка вероятности выбора признака. 

Что делает алгоритм? Он задает равномерное распределение вероятности выбора признака. В данном случае вероятность для каждого признака 0,01.

![20210423(4)](/home/ekaterina/Desktop/AO/Лекция 9/20210423(4).jpg)

Чтобы определится с **n**, каждый раз проверяем оптимальный набор признаков на качество классификации и если нас все устраивает, то выбираем минимальное, возможное число признаков.

Т.е. мы 10 раз выбрали случайный набор из 100 объемом 5. Для каждого из r наборов посчитали критерий качества (какой хотим), например, для каждой 5-ки признаков будем искать оптимальное направление Фишера, проецировать на него и считать, какой одномерный критерий Фишера. Те наборы, для которых оптимальное направление дает наибольший одномерный критерий Фишера – лучше и наоборот. Из r выбираем наилучший, для лучшего набора увеличиваем вероятность выбора в последующем **p->p+dp** этого признака. Для наихудшего набора, т.е. уменьшаем вероятность последующего выбора худших признаков **p->p-dp**.

Так проводим R групп испытаний.

После Rxr испытаний выбираем наилучшую подсистему из n признаков. Она часто, но не всегда, оказывается в конце.

Как влияет dp?

Если в нашем примере dp=0,01, то признак, попавший в наихудшую группу, то вероятность его будет равна 0 и больше мы его не увидим. Конечно, это плохо(выбрали подсистему из 10-ти признаков 9 из которых отвратительные и один попался замечательный и мы этот случайный замечательный признак убьем)

dp=0, т.е. никакой адаптации нет, по ходу работы алгоритма никак не меняются вероятности выбора признаков, поэтому алгоритм сводится к случайному поиску.

ПРОДОЛЖЕНИЕ СЛУДУЕТ…